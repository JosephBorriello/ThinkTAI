{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "  1/400 [..............................] - ETA: 2:34:54 - loss: 6.5240 - accuracy: 0.0000e+00"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 225\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[39m# If the user chose to train a new model, fit the model and save the best weights\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[39mif\u001b[39;00m user_input \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m1\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 225\u001b[0m     model\u001b[39m.\u001b[39;49mfit([sequences_input_train, sequences_output_train], sequences_output_train,\n\u001b[0;32m    226\u001b[0m             batch_size\u001b[39m=\u001b[39;49mBATCH_SIZE,\n\u001b[0;32m    227\u001b[0m             epochs\u001b[39m=\u001b[39;49mNUM_EPOCHS,\n\u001b[0;32m    228\u001b[0m             validation_data\u001b[39m=\u001b[39;49m([sequences_input_val, sequences_output_val], sequences_output_val),\n\u001b[0;32m    229\u001b[0m             callbacks\u001b[39m=\u001b[39;49m[checkpoint_callback, tensorboard_callback, early_stopping_callback])\n\u001b[0;32m    231\u001b[0m \u001b[39m# If the user chose to continue training from an existing model, load the saved weights and continue training\u001b[39;00m\n\u001b[0;32m    232\u001b[0m \u001b[39m# Please be aware that this requires you to have the exact same architecture as the one that was used when the model was saved\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[39melif\u001b[39;00m user_input \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m2\u001b[39m\u001b[39m'\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\jjbor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\jjbor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1734\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1735\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1736\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1739\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1740\u001b[0m ):\n\u001b[0;32m   1741\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1742\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1743\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1744\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\jjbor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\jjbor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\jjbor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    854\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    855\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    856\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    859\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    860\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\jjbor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    146\u001b[0m   (concrete_function,\n\u001b[0;32m    147\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 148\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    149\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\jjbor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1345\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1346\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1347\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1348\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1349\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function(\u001b[39m*\u001b[39;49margs))\n\u001b[0;32m   1350\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1351\u001b[0m     args,\n\u001b[0;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1353\u001b[0m     executing_eagerly)\n\u001b[0;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\jjbor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[0;32m    195\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 196\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[0;32m    198\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[0;32m    199\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[0;32m    200\u001b[0m     )\n\u001b[0;32m    201\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\u001b[39mself\u001b[39m, \u001b[39mlist\u001b[39m(args))\n",
      "File \u001b[1;32mc:\\Users\\jjbor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[0;32m   1456\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m   1458\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1459\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[0;32m   1460\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[0;32m   1461\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m   1462\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1463\u001b[0m   )\n\u001b[0;32m   1464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1466\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   1467\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1471\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[0;32m   1472\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\jjbor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries and modules from Tensorflow and Keras for building the neural network\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Bidirectional, Input, Concatenate, Layer\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import Callback, TensorBoard, EarlyStopping\n",
    "\n",
    "# Importing sklearn's train_test_split function for splitting the data into training and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Importing Tensorflow and Numpy for numerical operations and handling arrays\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Importing os module for handling file and directory paths\n",
    "import os\n",
    "\n",
    "# Import pygame for input ping\n",
    "import pygame\n",
    "\n",
    "# Defining paths to data file and model checkpoints\n",
    "DATA_PATH = './Data/data.txt'\n",
    "ENCODER_PATH = './Model/encoder_weights.h5'\n",
    "DECODER_PATH = './Model/decoder_weights.h5'\n",
    "\n",
    "# Setting hyperparameters for the model\n",
    "MAX_SEQ_LENGTH = 100\n",
    "EMBEDDING_DIM = 1024\n",
    "NUM_EPOCHS = 1000\n",
    "BATCH_SIZE = 8\n",
    "TEST_SPLIT_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "EARLY_STOPPING_PATIENCE = 5\n",
    "LSTM_UNITS = 1024\n",
    "LOG_DIR = './Logs'\n",
    "\n",
    "# Defining Attention layer as a custom Keras layer\n",
    "class Attention(Layer):\n",
    "    def __init__(self, units):\n",
    "        super(Attention, self).__init__()\n",
    "        # Dense layers for calculating attention scores\n",
    "        self.W1 = Dense(units)\n",
    "        self.W2 = Dense(units)\n",
    "        self.V = Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        # Expanding the hidden state dimension for addition operation with features\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        # Calculating attention scores\n",
    "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "# Defining a custom Keras callback for saving model weights\n",
    "class ModelCheckpoint(Callback):\n",
    "    def __init__(self, encoder_model, decoder_model):\n",
    "        super().__init__()\n",
    "        self.encoder_model = encoder_model\n",
    "        self.decoder_model = decoder_model\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current_val_loss = logs.get('val_loss')\n",
    "        # Save the model if it's the first epoch or if the validation loss has improved\n",
    "        if epoch == 0 or current_val_loss < self.best_val_loss: \n",
    "            self.best_val_loss = current_val_loss\n",
    "            self.encoder_model.save_weights(ENCODER_PATH)\n",
    "            self.decoder_model.save_weights(DECODER_PATH)\n",
    "\n",
    "# Function for sending an audible ping to the user\n",
    "def play_ping():\n",
    "    pygame.mixer.init()\n",
    "    pygame.mixer.music.load(\"./Sounds/notification_by_UNIVERSFIELD.mp3\")  # replace with the path to your sound file\n",
    "    pygame.mixer.music.play()\n",
    "\n",
    "# Function for prompting a user for input, notifying the user with an audible ping, and accepting and returning the input value\n",
    "def input_with_notification(prompt):\n",
    "    # Plays ping letting user know input is requested\n",
    "    play_ping()\n",
    "    # Prints prompt to screen before requesting user input\n",
    "    user_input = input(prompt)\n",
    "    # Returns input value\n",
    "    return user_input\n",
    "\n",
    "# Function for loading and preprocessing the data\n",
    "def load_and_preprocess_data(data_file):\n",
    "    # Raise an error if the data file does not exist\n",
    "    if not os.path.exists(data_file):\n",
    "        raise FileNotFoundError(f'The file {data_file} does not exist.')\n",
    "    data = []\n",
    "    # Open the data file and read lines\n",
    "    with open(data_file, 'r') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                # Split each line into description and subject and append to the data list\n",
    "                description, subject = line.strip().split('|')\n",
    "                data.append((description, subject))\n",
    "            except ValueError:\n",
    "                print(f'Skipped line due to wrong format: {line}')\n",
    "    # Shuffle the data for randomness\n",
    "    np.random.shuffle(data)\n",
    "    # Return the descriptions and subjects as separate lists\n",
    "    return [sample[0] for sample in data], [sample[1] for sample in data]\n",
    "\n",
    "# Function for tokenizing and padding the text data\n",
    "def tokenize_and_pad(texts, max_seq_length):\n",
    "    # Initialize a tokenizer with a special out-of-vocabulary token\n",
    "    tokenizer = Tokenizer(num_words=None, oov_token='<OOV>')\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    # Convert texts to sequences of integer tokens\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    # Pad the sequences to the maximum sequence length\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_seq_length, padding='post')\n",
    "    return tokenizer, padded_sequences\n",
    "\n",
    "# Function for building the model\n",
    "def build_model(max_seq_length, vocab_size_input, vocab_size_output, embedding_dim):\n",
    "    # Define the encoder input layer\n",
    "    encoder_input = Input(shape=(max_seq_length,))\n",
    "    encoder_embedding_layer = Embedding(vocab_size_input, embedding_dim)\n",
    "    encoder_embedding = encoder_embedding_layer(encoder_input)\n",
    "    # Define the encoder LSTM layer\n",
    "    encoder_lstm = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True, return_state=True))\n",
    "    encoder_output, forward_h, forward_c, backward_h, backward_c = encoder_lstm(encoder_embedding)\n",
    "    # Concatenate the forward and backward hidden states\n",
    "    state_h = Concatenate()([forward_h, backward_h])\n",
    "    state_c = Concatenate()([forward_c, backward_c])\n",
    "    # Define the decoder input layer\n",
    "    decoder_input = Input(shape=(max_seq_length,))\n",
    "    decoder_embedding_layer = Embedding(vocab_size_output, embedding_dim)\n",
    "    decoder_embedding = decoder_embedding_layer(decoder_input)\n",
    "    # Define the decoder LSTM layer\n",
    "    decoder_lstm = LSTM(LSTM_UNITS*2, return_sequences=True, return_state=True)\n",
    "    # Define the attention layer\n",
    "    attention_layer = Attention(LSTM_UNITS*2)\n",
    "    context_vector, attention_weights = attention_layer(encoder_output, state_h)\n",
    "    decoder_concat_input = Concatenate(axis=-1)([decoder_embedding, tf.repeat(tf.expand_dims(context_vector, 1), repeats=MAX_SEQ_LENGTH, axis=1)])\n",
    "    # Pass the concatenated input to the decoder LSTM\n",
    "    decoder_output, _, _ = decoder_lstm(decoder_concat_input, initial_state=[state_h, state_c])\n",
    "    # Define the output layer\n",
    "    decoder_output = Dense(vocab_size_output, activation='softmax')(decoder_output)\n",
    "    # Define the model with the encoder and decoder inputs and the decoder output\n",
    "    model = Model(inputs=[encoder_input, decoder_input], outputs=decoder_output)\n",
    "    return model, encoder_input, encoder_output, state_h, state_c, decoder_input, decoder_embedding_layer, decoder_lstm, attention_layer\n",
    "\n",
    "# Function for decoding a sequence of tokens into text\n",
    "def decode_sequence(input_sequence):\n",
    "    # Pass the input sequence to the encoder model and get the output and states\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_sequence, verbose=0)\n",
    "    states_value = [e_h, e_c]\n",
    "    # Initialize the target sequence with the start token\n",
    "    target_sequence = np.zeros((1, 1))\n",
    "    target_sequence[0, 0] = 1\n",
    "    output_sequence = ''\n",
    "    while True:\n",
    "        # Pass the target sequence and states to the decoder model and get the output tokens and new states\n",
    "        output_tokens, h, c = decoder_model.predict([target_sequence] + states_value + [e_out], verbose=0)\n",
    "        # Get the token with the highest probability\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        # Convert the token into a word\n",
    "        sampled_word = int_to_word_decoder.get(sampled_token_index, '<OOV>')  # dealing with the OOV token\n",
    "        output_sequence += ' ' + sampled_word\n",
    "        # Stop if the end token is found or the maximum sequence length is reached\n",
    "        if sampled_word == '<end>' or len(output_sequence) > MAX_SEQ_LENGTH:\n",
    "            break\n",
    "        # Update the target sequence and states for the next iteration\n",
    "        target_sequence = np.zeros((1, 1))\n",
    "        target_sequence[0, 0] = sampled_token_index\n",
    "        states_value = [h, c]\n",
    "    return output_sequence\n",
    "\n",
    "# Load and preprocess the data\n",
    "descriptions, subjects = load_and_preprocess_data(DATA_PATH)\n",
    "# Tokenize and pad the descriptions and subjects\n",
    "tokenizer_input, sequences_input = tokenize_and_pad(descriptions, MAX_SEQ_LENGTH)\n",
    "tokenizer_output, sequences_output = tokenize_and_pad(subjects, MAX_SEQ_LENGTH)\n",
    "# Get the vocab sizes for the input and output\n",
    "vocab_size_input = len(tokenizer_input.word_index) + 1\n",
    "vocab_size_output = len(tokenizer_output.word_index) + 1\n",
    "# Build the model\n",
    "model, encoder_input, encoder_output, state_h, state_c, decoder_input, decoder_embedding_layer, decoder_lstm, attention_layer = build_model(\n",
    "    MAX_SEQ_LENGTH, vocab_size_input, vocab_size_output, EMBEDDING_DIM)\n",
    "\n",
    "# Compile the model with a loss function, optimizer, and metrics\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Split the sequences into training and validation sets\n",
    "sequences_input_train, sequences_input_val, sequences_output_train, sequences_output_val = train_test_split(\n",
    "    sequences_input, np.expand_dims(sequences_output, -1), test_size=TEST_SPLIT_SIZE, random_state=RANDOM_STATE)\n",
    "\n",
    "# Create directories for saving model weights if they do not exist\n",
    "if not os.path.exists(os.path.dirname(ENCODER_PATH)):\n",
    "    os.makedirs(os.path.dirname(ENCODER_PATH))\n",
    "if not os.path.exists(os.path.dirname(DECODER_PATH)):\n",
    "    os.makedirs(os.path.dirname(DECODER_PATH))\n",
    "\n",
    "# Define TensorBoard and early stopping callbacks\n",
    "tensorboard_callback = TensorBoard(log_dir=LOG_DIR)\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=EARLY_STOPPING_PATIENCE)\n",
    "\n",
    "# Ask the user whether to train a new model, load an existing model, or continue training from an existing model\n",
    "user_input = input_with_notification(\"Enter '1' to Train New Model, '2' to Continue Training from Existing Model, or '3' to Load Existing Model for Testing Only: \")\n",
    "\n",
    "# Define the encoder and decoder models for inference\n",
    "encoder_model = Model(encoder_input, [encoder_output, state_h, state_c])\n",
    "decoder_state_input_h = Input(shape=(LSTM_UNITS*2,))\n",
    "decoder_state_input_c = Input(shape=(LSTM_UNITS*2,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_hidden_state_input = Input(shape=(MAX_SEQ_LENGTH, LSTM_UNITS*2))\n",
    "decoder_input_inf = Input(shape=(1,))\n",
    "decoder_embedding_inf = decoder_embedding_layer(decoder_input_inf)\n",
    "context_vector, _ = attention_layer(decoder_hidden_state_input, decoder_state_input_h)\n",
    "decoder_concat_input = Concatenate(axis=-1)([decoder_embedding_inf, tf.expand_dims(context_vector, 1)])\n",
    "decoder_outputs_inf, state_h_inf, state_c_inf = decoder_lstm(decoder_concat_input, initial_state=decoder_states_inputs)\n",
    "decoder_states_inf = [state_h_inf, state_c_inf]\n",
    "decoder_outputs_inf = Dense(vocab_size_output, activation='softmax')(decoder_outputs_inf)\n",
    "decoder_model = Model([decoder_input_inf] + decoder_states_inputs + [decoder_hidden_state_input], [decoder_outputs_inf] + decoder_states_inf)\n",
    "\n",
    "# Define the model checkpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(encoder_model, decoder_model)\n",
    "\n",
    "# If the user chose to train a new model, fit the model and save the best weights\n",
    "if user_input == '1':\n",
    "    model.fit([sequences_input_train, sequences_output_train], sequences_output_train,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            epochs=NUM_EPOCHS,\n",
    "            validation_data=([sequences_input_val, sequences_output_val], sequences_output_val),\n",
    "            callbacks=[checkpoint_callback, tensorboard_callback, early_stopping_callback])\n",
    "\n",
    "# If the user chose to continue training from an existing model, load the saved weights and continue training\n",
    "# Please be aware that this requires you to have the exact same architecture as the one that was used when the model was saved\n",
    "elif user_input == '2':\n",
    "    encoder_model.load_weights(ENCODER_PATH)\n",
    "    decoder_model.load_weights(DECODER_PATH)\n",
    "    model.fit([sequences_input_train, sequences_output_train], sequences_output_train,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            epochs=NUM_EPOCHS,\n",
    "            validation_data=([sequences_input_val, sequences_output_val], sequences_output_val),\n",
    "            callbacks=[checkpoint_callback, tensorboard_callback, early_stopping_callback])\n",
    "\n",
    "# If the user chose to load an existing model for testing, load the saved weights\n",
    "elif user_input == '3':\n",
    "    encoder_model.load_weights(ENCODER_PATH)\n",
    "    decoder_model.load_weights(DECODER_PATH)\n",
    "\n",
    "# Create a dictionary for converting integer tokens back into words\n",
    "int_to_word_decoder = {i: word for word, i in tokenizer_output.word_index.items()}\n",
    "int_to_word_decoder[1] = '<OOV>'\n",
    "\n",
    "# Loop for getting user input and predicting the output\n",
    "while True:\n",
    "    # Ask the user for a description\n",
    "    input_description = input_with_notification(\"Enter a new description (type 'quit' to exit): \")\n",
    "    # Break the loop if the user types 'quit'\n",
    "    if input_description.lower() == 'quit':\n",
    "        break\n",
    "    # Tokenize and pad the user's description\n",
    "    input_sequence = tokenizer_input.texts_to_sequences([input_description])\n",
    "    input_sequence = pad_sequences(input_sequence, maxlen=MAX_SEQ_LENGTH, padding='post')\n",
    "    # Decode the model's output sequence\n",
    "    predicted_sequence = decode_sequence(input_sequence)\n",
    "    # Print the predicted subject\n",
    "    print('Predicted Subject:', predicted_sequence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
