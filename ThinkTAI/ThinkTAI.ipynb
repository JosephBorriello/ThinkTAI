{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Global Paths\n",
    "SP_MODEL_PATH = \"spm_model.model\"\n",
    "MODEL_PATH = \"model_weights.pth\"\n",
    "DATA_FILE = \"data.xlsx\"\n",
    "PLAIN_TEXT_FILE = \"data.txt\"\n",
    "\n",
    "# Global Parameters\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "D_MODEL = 4096  # Dimensionality of the model's input and output features.\n",
    "N_HEAD = 128  # Number of attention heads.\n",
    "NUM_LAYERS = 96  # Number of transformer layers.\n",
    "DIM_FEEDFORWARD = 16384  # Dimensionality of the feed-forward neural networks.\n",
    "DROPOUT = 0.1  # Dropout probability.\n",
    "MAX_LEN = 5000  # Maximum sequence length.\n",
    "INPUT_DIM = 50257  # Determines the number of unique tokens in the input language.\n",
    "OUTPUT_DIM = 50257  # Determines the number of unique tokens in the output language.\n",
    "BATCH_SIZE = 8  # Number of samples processed in parallel.\n",
    "SRC_SEQ_LENGTH = 1024  # Determines the maximum length of the input sequence the model can handle.\n",
    "TGT_SEQ_LENGTH = 128  # Determines the maximum length of the output sequence the model can generate.\n",
    "\n",
    "# Convert Excel to plain text file\n",
    "df = pd.read_excel(DATA_FILE)\n",
    "html_data = df['HTML'].tolist()\n",
    "\n",
    "# Write HTML data to plain text file\n",
    "with open(PLAIN_TEXT_FILE, 'w', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows([[html] for html in html_data])\n",
    "\n",
    "# Initialize SentencePiece tokenizer\n",
    "if os.path.isfile(SP_MODEL_PATH):\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.Load(SP_MODEL_PATH)\n",
    "else:\n",
    "    # Train SentencePiece tokenizer on your dataset\n",
    "    spm.SentencePieceTrainer.Train(f\"--input={PLAIN_TEXT_FILE} --model_prefix=spm_model --vocab_size=15000\")\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.Load(SP_MODEL_PATH)\n",
    "    sp.Save(SP_MODEL_PATH)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "# Inference loop with beam search\n",
    "def inference(model, src, beam_width=5, max_length=100):\n",
    "    model.eval()\n",
    "    src = src.unsqueeze(0).to(device)\n",
    "    src = src.repeat(beam_width, 1)  # Repeat source sequence for beam search\n",
    "\n",
    "    with torch.no_grad():\n",
    "        src_encoding = model.embedding(src) * math.sqrt(D_MODEL)\n",
    "        src_encoding = model.pos_encoder(src_encoding)\n",
    "        memory = model.transformer_encoder(src_encoding)\n",
    "\n",
    "        tgt = torch.ones(beam_width, 1).long().to(device)  # Initialize target sequence with start token\n",
    "        tgt_lengths = torch.ones(beam_width).long().to(device)  # Initialize target sequence lengths\n",
    "        eos_flags = torch.zeros(beam_width).byte().to(device)  # Flags to track if beam search paths have reached end-of-sequence\n",
    "\n",
    "        scores_beam = torch.zeros(beam_width).to(device)  # Initialize scores_beam tensor\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            tgt_encoding = model.embedding(tgt) * math.sqrt(D_MODEL)\n",
    "            tgt_encoding = model.pos_encoder(tgt_encoding)\n",
    "            output = model.transformer_decoder(tgt_encoding, memory)\n",
    "\n",
    "            output = model.decoder(output[:, -1, :])  # Get logits for the last token\n",
    "            output = F.log_softmax(output, dim=-1)\n",
    "\n",
    "            output = output.view(beam_width, -1, OUTPUT_DIM)  # Reshape logits for beam search\n",
    "\n",
    "            if _ == 0:\n",
    "                scores, candidates = output.topk(beam_width, dim=-1)\n",
    "            else:\n",
    "                scores, candidates = output.topk(beam_width, dim=-1)\n",
    "                scores = scores + scores_beam.unsqueeze(2)  # Add scores of previous beam search paths\n",
    "\n",
    "            scores = scores.view(beam_width, -1)  # Reshape scores for beam search\n",
    "            candidates = candidates.view(beam_width, -1)  # Reshape candidates for beam search\n",
    "\n",
    "            if _ == 0:\n",
    "                scores_flat = scores.squeeze()\n",
    "            else:\n",
    "                scores_flat = scores.view(-1)  # Flatten scores for beam search\n",
    "\n",
    "            scores_beam, indices_beam = scores_flat.topk(beam_width, dim=-1)\n",
    "\n",
    "            tgt_candidates = candidates.view(-1)  # Flatten candidates for beam search\n",
    "            tgt_candidates_beam = tgt_candidates[indices_beam]  # Select candidates for beam search\n",
    "\n",
    "            tgt = torch.cat((tgt, tgt_candidates_beam.unsqueeze(1)), dim=1)  # Append selected candidates to target sequence\n",
    "\n",
    "            eos_flags = eos_flags | (tgt_candidates_beam == 1)  # Check if any of the selected candidates is the end token\n",
    "            if eos_flags.all():  # Break if all beam search paths have reached end-of-sequence\n",
    "                break\n",
    "\n",
    "            tgt_lengths = tgt_lengths + (~eos_flags).long()  # Update target sequence lengths\n",
    "\n",
    "        best_sequence_index = scores_beam.argmax().item()\n",
    "        best_sequence = tgt[best_sequence_index].tolist()\n",
    "\n",
    "    return best_sequence[1:]  # Remove start token\n",
    "\n",
    "\n",
    "class ThinkTAI(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, pretrained_weights=None):\n",
    "        super(ThinkTAI, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, D_MODEL)\n",
    "        self.pos_encoder = PositionalEncoding(D_MODEL, DROPOUT, max_len=MAX_LEN)\n",
    "\n",
    "        encoder_layers = nn.TransformerEncoderLayer(D_MODEL, N_HEAD, DIM_FEEDFORWARD, DROPOUT)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, NUM_LAYERS)\n",
    "\n",
    "        decoder_layers = nn.TransformerDecoderLayer(D_MODEL, N_HEAD, DIM_FEEDFORWARD, DROPOUT)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layers, NUM_LAYERS)\n",
    "\n",
    "        self.decoder = nn.Linear(D_MODEL, output_dim)\n",
    "\n",
    "        self.init_weights(pretrained_weights)\n",
    "\n",
    "    def init_weights(self, pretrained_weights=None):\n",
    "        if pretrained_weights is not None:\n",
    "            self.load_state_dict(torch.load(pretrained_weights))\n",
    "        else:\n",
    "            initrange = 0.1\n",
    "            self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "            self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src = self.embedding(src) * math.sqrt(D_MODEL)\n",
    "        src = self.pos_encoder(src)\n",
    "        memory = self.transformer_encoder(src)\n",
    "        tgt = self.embedding(tgt) * math.sqrt(D_MODEL)\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "        output = self.transformer_decoder(tgt, memory)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "def preprocess_data(input_data, target_data):\n",
    "    input_data = [tokenize_sentence(sentence) for sentence in input_data]\n",
    "    target_data = [tokenize_sentence(sentence) for sentence in target_data]\n",
    "\n",
    "    input_data = [torch.tensor(tokens) for tokens in input_data]\n",
    "    target_data = [torch.tensor(tokens) for tokens in target_data]\n",
    "\n",
    "    return input_data, target_data\n",
    "\n",
    "\n",
    "def tokenize_sentence(sentence):\n",
    "    return sp.EncodeAsIds(sentence)\n",
    "\n",
    "\n",
    "def pad_sequence_to_length(sequence, target_length, padding_token):\n",
    "    if len(sequence) < target_length:\n",
    "        # Pad the sequence to the target length\n",
    "        pad_length = target_length - len(sequence)\n",
    "        sequence = sequence + [padding_token] * pad_length\n",
    "\n",
    "    return sequence\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_sequences = []\n",
    "    tgt_sequences = []\n",
    "    for src, tgt in batch:\n",
    "        src_sequences.append(src)\n",
    "        tgt_sequences.append(tgt)\n",
    "\n",
    "    max_len = max(len(seq) for seq in src_sequences + tgt_sequences)\n",
    "    src_padded = pad_sequence([torch.tensor(pad_sequence_to_length(seq, max_len, ' ')) for seq in src_sequences],\n",
    "                              batch_first=True)\n",
    "    tgt_padded = pad_sequence([torch.tensor(pad_sequence_to_length(seq, max_len, ' ')) for seq in tgt_sequences],\n",
    "                              batch_first=True)\n",
    "    return src_padded, tgt_padded\n",
    "\n",
    "\n",
    "# Load and preprocess your dataset\n",
    "df = pd.read_excel(DATA_FILE)\n",
    "input_data = df['HTML'].tolist()\n",
    "target_data = df['Subject'].tolist()\n",
    "\n",
    "input_data, target_data = preprocess_data(input_data, target_data)\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "input_train, input_val_test, target_train, target_val_test = train_test_split(input_data, target_data, test_size=0.2,\n",
    "                                                                              random_state=RANDOM_SEED)\n",
    "input_val, input_test, target_val, target_test = train_test_split(input_val_test, target_val_test, test_size=0.5,\n",
    "                                                                  random_state=RANDOM_SEED)\n",
    "\n",
    "input_train = [tokenize_sentence(seq) for seq in input_train]\n",
    "target_train = [tokenize_sentence(seq) for seq in target_train]\n",
    "input_val = [tokenize_sentence(seq) for seq in input_val]\n",
    "target_val = [tokenize_sentence(seq) for seq in target_val]\n",
    "input_test = [tokenize_sentence(seq) for seq in input_test]\n",
    "target_test = [tokenize_sentence(seq) for seq in target_test]\n",
    "\n",
    "input_train = [torch.tensor(tokens) for tokens in input_train]\n",
    "target_train = [torch.tensor(tokens) for tokens in target_train]\n",
    "input_val = [torch.tensor(tokens) for tokens in input_val]\n",
    "target_val = [torch.tensor(tokens) for tokens in target_val]\n",
    "input_test = [torch.tensor(tokens) for tokens in input_test]\n",
    "target_test = [torch.tensor(tokens) for tokens in target_test]\n",
    "\n",
    "train_dataset = list(zip(input_train, target_train))\n",
    "val_dataset = list(zip(input_val, target_val))\n",
    "test_dataset = list(zip(input_test, target_test))\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = ThinkTAI(len(sp), len(sp), pretrained_weights=MODEL_PATH) if os.path.isfile(MODEL_PATH) else ThinkTAI(len(sp), len(sp))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "lr_scheduler = ReduceLROnPlateau(optimizer, patience=3)\n",
    "\n",
    "# Initialize SummaryWriter for TensorBoard logging\n",
    "writer = SummaryWriter(log_dir=\"logs\")\n",
    "\n",
    "# Training loop\n",
    "NUM_EPOCHS = 10\n",
    "best_val_loss = float('inf')\n",
    "early_stop_counter = 0\n",
    "early_stop_patience = 5\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for src, tgt in train_loader:\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt[:, :-1])\n",
    "        loss = F.cross_entropy(output.view(-1, output.shape[-1]), tgt[:, 1:].contiguous().view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in val_loader:\n",
    "            src = src.to(device)\n",
    "            tgt = tgt.to(device)\n",
    "\n",
    "            output = model(src, tgt[:, :-1])\n",
    "            loss = F.cross_entropy(output.view(-1, output.shape[-1]), tgt[:, 1:].contiguous().view(-1))\n",
    "            val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        writer.add_scalar(\"Loss/Train\", train_loss, epoch)\n",
    "        writer.add_scalar(\"Loss/Validation\", val_loss, epoch)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            early_stop_counter = 0\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "\n",
    "        if early_stop_counter >= early_stop_patience:\n",
    "            break\n",
    "\n",
    "    lr_scheduler.step(val_loss)  # Update learning rate based on validation loss\n",
    "\n",
    "    print(f\"Epoch: {epoch + 1}/{NUM_EPOCHS} | Train Loss: {train_loss:.3f} | Val Loss: {val_loss:.3f}\")\n",
    "\n",
    "# Test loop\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for src, tgt in test_loader:\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "\n",
    "        output = model(src, tgt[:, :-1])\n",
    "        loss = F.cross_entropy(output.view(-1, output.shape[-1]), tgt[:, 1:].contiguous().view(-1))\n",
    "        test_loss += loss.item()\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.3f}\")\n",
    "\n",
    "# Example usage\n",
    "input_sequence = \"Hello, how are you?\"\n",
    "input_tokens = tokenize_sentence(input_sequence)\n",
    "input_tokens = torch.LongTensor(input_tokens).unsqueeze(0).to(device)\n",
    "\n",
    "output_tokens = inference(model, input_tokens)\n",
    "\n",
    "output_sequence = sp.DecodeIds(output_tokens)\n",
    "print(output_sequence)\n",
    "\n",
    "# Close the SummaryWriter\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
