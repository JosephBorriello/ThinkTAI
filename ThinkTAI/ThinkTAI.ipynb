{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jjbor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchtext\\data\\utils.py:105: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Discover the joy of driving a brand-new 2023 Kia Sorento at Sunrise Kia. Available in the LX trim level, this Sorento is set to redefine your driving experience.', 'NEW 2023 Kia Sorento LX')\n",
      "(\"At Elite Autos, experience the elegance of a pre-owned 2017 BMW 5 Series. Crafted in 2017, this 5 Series comes with a 530i trim level, a true testament to BMW's luxury and performance.\", 'USED 2017 BMW 5 Series 530i')\n",
      "('At Sunshine Honda, be the first to own a new 2023 Honda Pilot. This Pilot is a masterpiece, crafted in the EX-L trim level, promising to make every ride an adventure.', 'NEW 2023 Honda Pilot EX-L')\n",
      "(\"At Silverline Autos, experience the grandeur of 2015 with a pre-owned Lexus NX. This NX, in the 200t trim level, is a symbol of 2015's luxury and design.\", 'USED 2015 Lexus NX 200t')\n",
      "('At Vintage Autos, get a taste of 2019 with a pre-owned Toyota Highlander. This Highlander comes in the LE trim level and represents the best of what 2019 had to offer.', 'USED 2019 Toyota Highlander LE')\n",
      "(\"Dreaming of owning a Subaru? At Mountain Motors, we have a 2019 pre-loved Subaru Forester, which comes with a superior Sport trim level. Don't miss this golden opportunity.\", 'USED 2019 Subaru Forester Sport')\n",
      "(\"Discover the charm of 2018 with a pre-owned Honda Fit at CitySide Honda. This Fit comes with a LX trim level and is a reminder of 2018's innovation and compact design.\", 'USED 2018 Honda Fit LX')\n",
      "('Experience the future of driving with a 2023 Audi Q3 at Majestic Motors. This Q3 is fresh off the assembly line and comes with a Premium trim level, ready to deliver an unparalleled driving experience.', 'NEW 2023 Audi Q3 Premium')\n",
      "('Step into Galaxy Toyota and discover a fresh 2023 Toyota Camry. With an SE trim level, this Camry promises an unprecedented level of comfort and innovation.', 'NEW 2023 Toyota Camry SE')\n",
      "('At USA Motors, discover the thrill of driving a 2023 GMC Sierra 1500. This Sierra 1500 is brand new and comes with a SLE trim level, ready to redefine your driving experience.', 'NEW 2023 GMC Sierra 1500 SLE')\n",
      "(\"Travel back to 2017 at Elite Autos with a pre-owned Chevrolet Traverse. This Traverse, in the LT trim level, represents the best of 2017's family-friendly design and comfort.\", 'USED 2017 Chevrolet Traverse LT')\n",
      "('At Green Leaf Motors, take a journey back to 2020 with a pre-owned Honda HR-V. This HR-V, in the LX trim level, represents the best of what 2020 had to offer.', 'USED 2020 Honda HR-V LX')\n",
      "('Discover the future with a 2023 Volkswagen Jetta at German Precision Autos. This Jetta is fresh off the assembly line and comes with a S trim level, promising unmatched style and comfort.', 'NEW 2023 Volkswagen Jetta S')\n",
      "(\"At CitySide Honda, discover the joy of driving a brand-new Honda Accord. It's a 2023 model and comes with a well-crafted LX trim level. Start your journey today.\", 'NEW 2023 Honda Accord LX')\n",
      "('Live the legacy of 2019 with a pre-owned Audi Q5 at Majestic Motors. This Q5 comes in the Premium Plus trim level, ready to elevate your driving experience.', 'USED 2019 Audi Q5 Premium Plus')\n",
      "('Experience the elegance of a pre-owned 2018 Lexus RX at Silverline Autos. Crafted in 2018, this Lexus RX comes with a 350 trim level and is a true mark of luxury.', 'USED 2018 Lexus RX 350')\n",
      "('Redefine your driving experience with a pre-owned 2017 Audi A4 from Elite Autos. This magnificent vehicle was engineered in 2017 and comes in the Premium trim level. A true symbol of luxury and power.', 'USED 2017 Audi A4 Premium')\n",
      "('At Trailblazer Motors, experience the power of a brand new 2023 Jeep Compass. This Compass, in the Sport trim level, is ready to redefine your off-road experience.', 'NEW 2023 Jeep Compass Sport')\n",
      "('Take a journey back to 2016 at Vintage Autos with a pre-owned BMW X3. Equipped with the xDrive28i trim level, this BMW X3 truly represents the blend of luxury and performance.', 'USED 2016 BMW X3 xDrive28i')\n",
      "('Step into the future with a 2023 Ford Escape at Blue Sky Ford. In the SE trim level, this Escape is prepared to take you on any adventure.', 'NEW 2023 Ford Escape SE')\n",
      "('At Sunnyside Autos, we are proud to present a pre-owned 2015 Honda CR-V. This CR-V comes with an EX-L trim level and was first unveiled in 2015. Own a piece of the Honda history.', 'USED 2015 Honda CR-V EX-L')\n",
      "('Step into 2023 with a new Kia Soul from Sunrise Kia. This Soul comes with a LX trim level and is ready to redefine your driving experience.', 'NEW 2023 Kia Soul LX')\n",
      "('Discover the future with a brand new 2023 Nissan Altima at New Horizon Autos. This Altima, crafted in the S trim level, promises to redefine your driving experience.', 'NEW 2023 Nissan Altima S')\n",
      "('Experience the future of driving with a 2023 Chevrolet Colorado at USA Motors. This Colorado is fresh off the production line and comes with a WT trim level, ready to take on any challenge.', 'NEW 2023 Chevrolet Colorado WT')\n",
      "('Get ready to rule the roads in a brand new 2023 Hyundai Santa Fe at Rainbow Hyundai. This Santa Fe comes with a SE trim level and promises a whole new level of driving pleasure.', 'NEW 2023 Hyundai Santa Fe SE')\n",
      "('Start your journey in a brand new 2023 Subaru Forester at Mountain Motors. This Forester, fresh off the production line, comes with a Premium trim level, promising to make every trip a pleasure.', 'NEW 2023 Subaru Forester Premium')\n",
      "('Relive the elegance of 2019 with a pre-owned Toyota Avalon at Galaxy Toyota. This Avalon, with a XLE trim level, is a testament to the luxury and style of 2019.', 'USED 2019 Toyota Avalon XLE')\n",
      "('Own a piece of 2016 with a pre-owned Audi A6 at Elite Autos. This A6 comes with a Premium trim level and stands as a symbol of the luxury that was 2016.', 'USED 2016 Audi A6 Premium')\n",
      "('Drive home a piece of 2020 with a pre-owned Subaru Outback from Mountain Motors. With a Premium trim level, this Outback is designed to make every journey memorable.', 'USED 2020 Subaru Outback Premium')\n",
      "('Discover a new world with a 2023 Jeep Grand Cherokee at Trailblazer Motors. This Grand Cherokee is brand new and comes with a Laredo trim level, promising a level of comfort and adventure unlike any other.', 'NEW 2023 Jeep Grand Cherokee Laredo')\n",
      "('Discover a freshly minted 2023 Hyundai Sonata at Rainbow Hyundai. Adorned with an SE trim level, this Sonata promises a blend of style and innovation.', 'NEW 2023 Hyundai Sonata SE')\n",
      "('Step into the future at Sunshine Toyota with a brand new 2023 Toyota Tacoma. This Tacoma, crafted in the SR trim level, is ready to redefine your driving experience.', 'NEW 2023 Toyota Tacoma SR')\n",
      "('Get a taste of 2015 with a pre-owned Mercedes-Benz E-Class at Silverline Autos. This E-Class comes with an E 350 trim level and stands as a symbol of timeless elegance.', 'USED 2015 Mercedes-Benz E-Class E 350')\n",
      "(\"At Elite Autos, relive the charm of 2019 with a pre-owned BMW i3. This i3, in the Base trim level, represents the pinnacle of 2019's electric revolution.\", 'USED 2019 BMW i3 Base')\n",
      "(\"Discover the beauty of 2020 with a pre-owned Honda Insight at CitySide Honda. This Insight, in the LX trim level, represents the best of 2020's efficient design.\", 'USED 2020 Honda Insight LX')\n",
      "('At Silverline Autos, take a journey back to 2018 with a pre-owned Mercedes-Benz GLC. This GLC, in the GLC 300 trim level, represents the best of what 2018 had to offer.', 'USED 2018 Mercedes-Benz GLC GLC 300')\n",
      "('Get ready to rule the roads in a brand-new 2023 Chevrolet Malibu. Presented in LT trim level, this beauty is waiting for you at USA Motors. Be a part of the Chevrolet legacy.', 'NEW 2023 Chevrolet Malibu LT')\n",
      "('Relive the grandeur of 2018 with a pre-owned Mercedes-Benz C-Class at Prestige Benz. This C-Class comes in the C 300 trim level and speaks volumes about its luxury and power.', 'USED 2018 Mercedes-Benz C-Class C 300')\n",
      "('At Blue Sky Ford, get ready to command the roads in a 2023 Ford Explorer. This Explorer is brand new and comes in the XLT trim level, promising a driving experience like no other.', 'NEW 2023 Ford Explorer XLT')\n",
      "('New Horizons Autos is proud to present a 2023 Nissan Rogue in the SV trim level. Born in 2023, this Nissan Rogue is the epitome of style and comfort.', 'NEW 2023 Nissan Rogue SV')\n",
      "('At USA Motors, be the first to own a brand new 2023 Chevrolet Traverse. This Traverse, crafted in the LS trim level, promises to deliver a journey like no other.', 'NEW 2023 Chevrolet Traverse LS')\n",
      "(\"Relive the glory of 2017 with a pre-owned Lexus ES at Silverline Autos. This ES comes with a 350 trim level and represents the best of 2017's luxury and refinement.\", 'USED 2017 Lexus ES 350')\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'E:/GitHub/ThinkTAI/ThinkTAI/Model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 233\u001b[0m\n\u001b[0;32m    230\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m| Test Loss: \u001b[39m\u001b[39m{\u001b[39;00mtest_loss\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m | Test PPL: \u001b[39m\u001b[39m{\u001b[39;00mmath\u001b[39m.\u001b[39mexp(test_loss)\u001b[39m:\u001b[39;00m\u001b[39m7.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m |\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    232\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 233\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[1], line 206\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[39m# Load model if it exists\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(args\u001b[39m.\u001b[39mmodel_path):\n\u001b[1;32m--> 206\u001b[0m     model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39;49mload(args\u001b[39m.\u001b[39;49mmodel_path))\n\u001b[0;32m    207\u001b[0m     model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m    209\u001b[0m \u001b[39m# Define optimizer and loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jjbor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    789\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 791\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[0;32m    792\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    793\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    794\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    795\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m    796\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\jjbor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    270\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 271\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[0;32m    272\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    273\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\Users\\jjbor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[1;32m--> 252\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'E:/GitHub/ThinkTAI/ThinkTAI/Model'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data.metrics import bleu_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import spacy\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import heapq\n",
    "from torch.nn import functional as F\n",
    "import argparse\n",
    "import math\n",
    "\n",
    "DATA_FILE = r\"E:/GitHub/ThinkTAI/ThinkTAI/Data/data.txt\"\n",
    "MODEL_PATH = r\"E:/GitHub/ThinkTAI/ThinkTAI/Model\"\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "# Replace argparse section with this\n",
    "class Args:\n",
    "    def __init__(self, model_path, data_file):\n",
    "        self.model_path = model_path\n",
    "        self.data_file = data_file\n",
    "\n",
    "args = Args(MODEL_PATH, DATA_FILE)\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Loading the spacy tokenizer\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "tokenizer = get_tokenizer('spacy', language='en')\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "def tokenize_and_convert_to_indices(text, vocab):\n",
    "    tokens = tokenize_en(text)\n",
    "    return [vocab[tok] for tok in tokens]\n",
    "\n",
    "def yield_tokens(data_iter, tokenize_func):\n",
    "    for item in data_iter:\n",
    "        print(item)\n",
    "        text, _ = item\n",
    "        yield tokenize_func(text)\n",
    "\n",
    "\n",
    "def collate_fn(batch, vocab):\n",
    "    src_batch, trg_batch = [], []\n",
    "    for src_item, trg_item in batch:\n",
    "        src_batch.append(torch.tensor(src_item, dtype=torch.long))\n",
    "        trg_batch.append(torch.tensor(trg_item, dtype=torch.long))\n",
    "    src_batch = pad_sequence(src_batch, padding_value=vocab['<pad>'])\n",
    "    trg_batch = pad_sequence(trg_batch, padding_value=vocab['<pad>'])\n",
    "    return src_batch, trg_batch\n",
    "\n",
    "def build_vocab(file_path, tokenizer):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = f.read().splitlines()\n",
    "    vocab = build_vocab_from_iterator(yield_tokens(data))\n",
    "    # ensure <pad> token is in the vocab\n",
    "    if '<pad>' not in vocab:\n",
    "        vocab.append('<pad>')\n",
    "    return vocab\n",
    "\n",
    "def read_data(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = f.read().splitlines()\n",
    "    dataset = [(item.split('|')[0], item.split('|')[1]) for item in data]\n",
    "    return dataset\n",
    "\n",
    "def split_data(data, train_size=0.7, val_size=0.15, test_size=0.15):\n",
    "    assert train_size + val_size + test_size == 1.0, \"The sum of the split ratios should be 1.0\"\n",
    "\n",
    "    train_data, rest_data = train_test_split(data, test_size=1-train_size, random_state=SEED)\n",
    "    val_data, test_data = train_test_split(rest_data, test_size=test_size/(val_size + test_size), random_state=SEED)\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "def process_data(data, vocab):\n",
    "    dataset = [(tokenize_and_convert_to_indices(src, vocab), tokenize_and_convert_to_indices(trg, vocab)) for src, trg in data]\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=lambda x: collate_fn(x, vocab))\n",
    "    return dataloader\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "# Define your model here\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, d_model, nhead, num_layers, dim_feedforward, dropout, pad_idx):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)  # Added positional encoding\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.fc_out = nn.Linear(d_model, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src_mask = self.transformer.generate_square_subsequent_mask(src.size(0)).to(src.device)\n",
    "        trg_mask = self.transformer.generate_square_subsequent_mask(trg.size(0)).to(trg.device) & \\\n",
    "            (trg.unsqueeze(-1).permute(1, 0, 2) != self.pad_idx).type_as(src_mask)\n",
    "        src = self.embedding(src)\n",
    "        src = self.pos_encoder(src)  # Added positional encoding\n",
    "        trg = self.embedding(trg)\n",
    "        trg = self.pos_encoder(trg)  # Added positional encoding\n",
    "        src = src.permute(1, 0, 2)\n",
    "        trg = trg.permute(1, 0, 2)\n",
    "        output = self.transformer(src, trg, src_mask=src_mask, tgt_mask=trg_mask)\n",
    "        output = output.permute(1, 0, 2)\n",
    "        return self.fc_out(self.dropout(output))\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src, trg = batch\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg)\n",
    "        output_dim = output.shape[-1]\n",
    "        # Assumes that the first token is a start-of-sequence token that should be ignored by the loss\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src, trg = batch\n",
    "            output = model(src, trg)  # turn off teacher forcing\n",
    "            output_dim = output.shape[-1]\n",
    "            # Assumes that the first token is a start-of-sequence token that should be ignored by the loss\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def main():\n",
    "    train_data, val_data, test_data = split_data(read_data(DATA_FILE))\n",
    "\n",
    "    # Now, use these lists in place of reading files\n",
    "    SRC_VOCAB = build_vocab_from_iterator(yield_tokens(train_data, tokenize_en))\n",
    "    TRG_VOCAB = build_vocab_from_iterator(yield_tokens(val_data, tokenize_en))\n",
    "\n",
    "    PAD_IDX = SRC_VOCAB['<pad>']  # Assuming '<pad>' is your padding token\n",
    "\n",
    "    train_iterator = DataLoader(train_data, batch_size=BATCH_SIZE, collate_fn=lambda x: collate_fn(x, SRC_VOCAB))\n",
    "    valid_iterator = DataLoader(val_data, batch_size=BATCH_SIZE, collate_fn=lambda x: collate_fn(x, TRG_VOCAB))\n",
    "    test_iterator = DataLoader(test_data, batch_size=BATCH_SIZE, collate_fn=lambda x: collate_fn(x, TRG_VOCAB))\n",
    "\n",
    "    # Parameters for the model\n",
    "    INPUT_DIM = len(SRC_VOCAB)\n",
    "    OUTPUT_DIM = len(TRG_VOCAB)\n",
    "    D_MODEL = 256\n",
    "    NHEAD = 8\n",
    "    NUM_LAYERS = 3\n",
    "    DIM_FEEDFORWARD = 512\n",
    "    DROPOUT = 0.1\n",
    "    CLIP = 1.0\n",
    "\n",
    "    # Instantiate the model\n",
    "    model = TransformerModel(INPUT_DIM, OUTPUT_DIM, D_MODEL, NHEAD, NUM_LAYERS, DIM_FEEDFORWARD, DROPOUT, PAD_IDX)\n",
    "\n",
    "    # Use appropriate device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Load model if it exists\n",
    "    if os.path.exists(args.model_path):\n",
    "        model.load_state_dict(torch.load(args.model_path))\n",
    "        model = model.to(device)\n",
    "\n",
    "    # Define optimizer and loss\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)\n",
    "\n",
    "    # Training loop\n",
    "    N_EPOCHS = 10\n",
    "    best_valid_loss = float('inf')\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "        valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), args.model_path)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    test_loss = evaluate(model, test_iterator, criterion)\n",
    "    print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
