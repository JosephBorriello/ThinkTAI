{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # PyTorch is a popular deep learning framework.\n",
    "import torch.nn as nn  # Provides a set of building blocks for defining neural networks.\n",
    "import torch.optim as optim  # Implements various optimization algorithms for training models.\n",
    "import torch.nn.functional as F  # Contains functional interface for operations such as activation functions.\n",
    "import math  # Provides mathematical functions and constants.\n",
    "from torch.utils.data import DataLoader  # Helps with loading and batching data during training.\n",
    "from torch.utils.tensorboard import SummaryWriter  # Allows logging of training information for visualization in TensorBoard.\n",
    "from sklearn.model_selection import train_test_split  # Splits data into training and validation/test sets.\n",
    "import sentencepiece as spm  # A library for tokenization and subword encoding.\n",
    "import os  # Provides functions for interacting with the operating system.\n",
    "import numpy as np  # A powerful library for numerical computing.\n",
    "from torch.nn.utils.rnn import pad_sequence  # Helps with padding sequences to the same length in a batch.\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau  # Implements a learning rate scheduler that adjusts the learning rate based on validation loss.\n",
    "import pandas as pd  # A library for data manipulation and analysis.\n",
    "import csv  # Provides functionality for reading and writing CSV files.\n",
    "\n",
    "\n",
    "# Global Paths\n",
    "SP_MODEL_PATH = \"spm_model.model\"  # Path for SentencePiece model file\n",
    "MODEL_PATH = \"model_weights.pth\"  # Path for trained model weights file\n",
    "DATA_FILE = \"data.xlsx\"  # Path for input data file in Excel format\n",
    "PLAIN_TEXT_FILE = \"data.txt\"  # Path for plain text data file\n",
    "\n",
    "# Global Parameters\n",
    "RANDOM_SEED = 42  # Random seed for reproducibility\n",
    "\n",
    "D_MODEL = 4096  # Dimensionality of the model's input and output features.\n",
    "N_HEAD = 128  # Number of attention heads.\n",
    "NUM_LAYERS = 96  # Number of transformer layers.\n",
    "DIM_FEEDFORWARD = 16384  # Dimensionality of the feed-forward neural networks.\n",
    "DROPOUT = 0.1  # Dropout probability.\n",
    "MAX_LEN = 5000  # Maximum sequence length.\n",
    "INPUT_DIM = 50257  # Determines the number of unique tokens in the input language.\n",
    "OUTPUT_DIM = 50257  # Determines the number of unique tokens in the output language.\n",
    "BATCH_SIZE = 8  # Number of samples processed in parallel.\n",
    "SRC_SEQ_LENGTH = 1024  # Determines the maximum length of the input sequence the model can handle.\n",
    "TGT_SEQ_LENGTH = 128  # Determines the maximum length of the output sequence the model can generate.\n",
    "\n",
    "# Convert Excel to plain text file\n",
    "df = pd.read_excel(DATA_FILE)  # Read the Excel file into a pandas DataFrame\n",
    "html_data = df['HTML'].tolist()  # Extract the 'HTML' column as a list\n",
    "\n",
    "# Write HTML data to plain text file\n",
    "with open(PLAIN_TEXT_FILE, 'w', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)  # Create a CSV writer\n",
    "    writer.writerows([[html] for html in html_data])  # Write each HTML content as a separate row\n",
    "\n",
    "# Initialize SentencePiece tokenizer\n",
    "if os.path.isfile(SP_MODEL_PATH):\n",
    "    sp = spm.SentencePieceProcessor()  # Create a SentencePiece tokenizer instance\n",
    "    sp.Load(SP_MODEL_PATH)  # Load the existing SentencePiece model\n",
    "else:\n",
    "    # Train SentencePiece tokenizer on your dataset\n",
    "    spm.SentencePieceTrainer.Train(f\"--input={PLAIN_TEXT_FILE} --model_prefix=spm_model --vocab_size=15000\")  # Train the tokenizer with specified options\n",
    "    sp = spm.SentencePieceProcessor()  # Create a SentencePiece tokenizer instance\n",
    "    sp.Load(SP_MODEL_PATH)  # Load the trained SentencePiece model\n",
    "    sp.Save(SP_MODEL_PATH)  # Save the SentencePiece model for future use\n",
    "\n",
    "# Class for positional encoding in the transformer model\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)  # Create a tensor for positional encodings\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # Create a tensor for position indices\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))  # Create a tensor for division terms\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # Compute sine of even-indexed positions\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # Compute cosine of odd-indexed positions\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)  # Reshape the positional encodings\n",
    "        self.register_buffer('pe', pe)  # Register the positional encodings as a buffer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]  # Add positional encodings to the input\n",
    "        return self.dropout(x)  # Apply dropout to the output\n",
    "\n",
    "# Function for inference with beam search\n",
    "def inference(model, src, beam_width=5, max_length=100):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    src = src.unsqueeze(0).to(device)  # Add a batch dimension and move to the device\n",
    "    src = src.repeat(beam_width, 1)  # Repeat the source sequence for beam search\n",
    "\n",
    "    with torch.no_grad():\n",
    "        src_encoding = model.embedding(src) * math.sqrt(D_MODEL)  # Embed the source sequence\n",
    "        src_encoding = model.pos_encoder(src_encoding)  # Apply positional encoding to the source sequence\n",
    "        memory = model.transformer_encoder(src_encoding)  # Encode the source sequence\n",
    "\n",
    "        tgt = torch.ones(beam_width, 1).long().to(device)  # Initialize target sequence with start token\n",
    "        tgt_lengths = torch.ones(beam_width).long().to(device)  # Initialize target sequence lengths\n",
    "        eos_flags = torch.zeros(beam_width).byte().to(device)  # Flags to track if beam search paths have reached end-of-sequence\n",
    "\n",
    "        scores_beam = torch.zeros(beam_width).to(device)  # Initialize scores_beam tensor\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            tgt_encoding = model.embedding(tgt) * math.sqrt(D_MODEL)  # Embed the target sequence\n",
    "            tgt_encoding = model.pos_encoder(tgt_encoding)  # Apply positional encoding to the target sequence\n",
    "            output = model.transformer_decoder(tgt_encoding, memory)  # Decode the target sequence\n",
    "\n",
    "            output = model.decoder(output[:, -1, :])  # Get logits for the last token\n",
    "            output = F.log_softmax(output, dim=-1)  # Apply log softmax to convert logits to probabilities\n",
    "\n",
    "            output = output.view(beam_width, -1, OUTPUT_DIM)  # Reshape logits for beam search\n",
    "\n",
    "            if _ == 0:\n",
    "                scores, candidates = output.topk(beam_width, dim=-1)  # Get top-k scores and candidates\n",
    "            else:\n",
    "                scores, candidates = output.topk(beam_width, dim=-1)  # Get top-k scores and candidates\n",
    "                scores = scores + scores_beam.unsqueeze(2)  # Add scores of previous beam search paths\n",
    "\n",
    "            scores = scores.view(beam_width, -1)  # Reshape scores for beam search\n",
    "            candidates = candidates.view(beam_width, -1)  # Reshape candidates for beam search\n",
    "\n",
    "            if _ == 0:\n",
    "                scores_flat = scores.squeeze()  # Flatten scores for beam search\n",
    "            else:\n",
    "                scores_flat = scores.view(-1)  # Flatten scores for beam search\n",
    "\n",
    "            scores_beam, indices_beam = scores_flat.topk(beam_width, dim=-1)  # Get top-k scores and indices\n",
    "\n",
    "            tgt_candidates = candidates.view(-1)  # Flatten candidates for beam search\n",
    "            tgt_candidates_beam = tgt_candidates[indices_beam]  # Select candidates for beam search\n",
    "\n",
    "            tgt = torch.cat((tgt, tgt_candidates_beam.unsqueeze(1)), dim=1)  # Append selected candidates to target sequence\n",
    "\n",
    "            eos_flags = eos_flags | (tgt_candidates_beam == 1)  # Check if any of the selected candidates is the end token\n",
    "            if eos_flags.all():  # Break if all beam search paths have reached end-of-sequence\n",
    "                break\n",
    "\n",
    "            tgt_lengths = tgt_lengths + (~eos_flags).long()  # Update target sequence lengths\n",
    "\n",
    "        best_sequence_index = scores_beam.argmax().item()  # Find the index of the best sequence\n",
    "        best_sequence = tgt[best_sequence_index].tolist()  # Convert the best sequence to a list\n",
    "\n",
    "    return best_sequence[1:]  # Remove the start token from the best sequence\n",
    "\n",
    "# Class definition for the main ThinkTAI model\n",
    "class ThinkTAI(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, pretrained_weights=None):\n",
    "        super(ThinkTAI, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, D_MODEL)  # Embedding layer for input tokens\n",
    "        self.pos_encoder = PositionalEncoding(D_MODEL, DROPOUT, max_len=MAX_LEN)  # Positional encoding layer\n",
    "\n",
    "        encoder_layers = nn.TransformerEncoderLayer(D_MODEL, N_HEAD, DIM_FEEDFORWARD, DROPOUT)  # Encoder layers for the transformer\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, NUM_LAYERS)  # Transformer encoder\n",
    "\n",
    "        decoder_layers = nn.TransformerDecoderLayer(D_MODEL, N_HEAD, DIM_FEEDFORWARD, DROPOUT)  # Decoder layers for the transformer\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layers, NUM_LAYERS)  # Transformer decoder\n",
    "\n",
    "        self.decoder = nn.Linear(D_MODEL, output_dim)  # Linear layer for output prediction\n",
    "\n",
    "        self.init_weights(pretrained_weights)  # Initialize weights of the model\n",
    "\n",
    "    def init_weights(self, pretrained_weights=None):\n",
    "        if pretrained_weights is not None:\n",
    "            self.load_state_dict(torch.load(pretrained_weights))  # Load pretrained weights if available\n",
    "        else:\n",
    "            initrange = 0.1\n",
    "            self.embedding.weight.data.uniform_(-initrange, initrange)  # Initialize embedding weights uniformly\n",
    "            self.decoder.weight.data.uniform_(-initrange, initrange)  # Initialize decoder weights uniformly\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src = self.embedding(src) * math.sqrt(D_MODEL)  # Embed the source sequence\n",
    "        src = self.pos_encoder(src)  # Apply positional encoding to the source sequence\n",
    "        memory = self.transformer_encoder(src)  # Encode the source sequence\n",
    "        tgt = self.embedding(tgt) * math.sqrt(D_MODEL)  # Embed the target sequence\n",
    "        tgt = self.pos_encoder(tgt)  # Apply positional encoding to the target sequence\n",
    "        output = self.transformer_decoder(tgt, memory)  # Decode the target sequence\n",
    "        output = self.decoder(output)  # Predict the output\n",
    "        return output\n",
    "\n",
    "# Function to preprocess input and target data\n",
    "def preprocess_data(input_data, target_data):\n",
    "    input_data = [tokenize_sentence(sentence) for sentence in input_data]  # Tokenize input sentences\n",
    "    target_data = [tokenize_sentence(sentence) for sentence in target_data]  # Tokenize target sentences\n",
    "\n",
    "    input_data = [torch.tensor(tokens) for tokens in input_data]  # Convert input tokens to tensors\n",
    "    target_data = [torch.tensor(tokens) for tokens in target_data]  # Convert target tokens to tensors\n",
    "\n",
    "    return input_data, target_data\n",
    "\n",
    "# Function to tokenize a sentence using SentencePiece tokenizer\n",
    "def tokenize_sentence(sentence):\n",
    "    return sp.EncodeAsIds(sentence)\n",
    "\n",
    "# Function to pad a sequence to a target length with a padding token\n",
    "def pad_sequence_to_length(sequence, target_length, padding_token):\n",
    "    if len(sequence) < target_length:\n",
    "        # Pad the sequence to the target length\n",
    "        pad_length = target_length - len(sequence)\n",
    "        sequence = sequence + [padding_token] * pad_length\n",
    "\n",
    "    return sequence\n",
    "\n",
    "# Function to collate a batch of sequences\n",
    "def collate_fn(batch):\n",
    "    src_sequences = []\n",
    "    tgt_sequences = []\n",
    "    for src, tgt in batch:\n",
    "        src_sequences.append(src)\n",
    "        tgt_sequences.append(tgt)\n",
    "\n",
    "    max_len = max(len(seq) for seq in src_sequences + tgt_sequences)  # Compute the maximum sequence length\n",
    "    src_padded = pad_sequence([torch.tensor(pad_sequence_to_length(seq, max_len, ' ')) for seq in src_sequences],\n",
    "                              batch_first=True)  # Pad and create a batch of source sequences\n",
    "    tgt_padded = pad_sequence([torch.tensor(pad_sequence_to_length(seq, max_len, ' ')) for seq in tgt_sequences],\n",
    "                              batch_first=True)  # Pad and create a batch of target sequences\n",
    "    return src_padded, tgt_padded\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "df = pd.read_excel(DATA_FILE)  # Read the Excel file into a pandas DataFrame\n",
    "input_data = df['HTML'].tolist()  # Extract the 'HTML' column as a list\n",
    "target_data = df['Subject'].tolist()  # Extract the 'Subject' column as a list\n",
    "\n",
    "input_data, target_data = preprocess_data(input_data, target_data)  # Preprocess the input and target data\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "input_train, input_val_test, target_train, target_val_test = train_test_split(input_data, target_data, test_size=0.2,\n",
    "                                                                              random_state=RANDOM_SEED)  # Split the data into train and validation/test sets\n",
    "input_val, input_test, target_val, target_test = train_test_split(input_val_test, target_val_test, test_size=0.5,\n",
    "                                                                  random_state=RANDOM_SEED)  # Split the validation/test set into validation and test sets\n",
    "\n",
    "input_train = [tokenize_sentence(seq) for seq in input_train]  # Tokenize the input train sequences\n",
    "target_train = [tokenize_sentence(seq) for seq in target_train]  # Tokenize the target train sequences\n",
    "input_val = [tokenize_sentence(seq) for seq in input_val]  # Tokenize the input validation sequences\n",
    "target_val = [tokenize_sentence(seq) for seq in target_val]  # Tokenize the target validation sequences\n",
    "input_test = [tokenize_sentence(seq) for seq in input_test]  # Tokenize the input test sequences\n",
    "target_test = [tokenize_sentence(seq) for seq in target_test]  # Tokenize the target test sequences\n",
    "\n",
    "input_train = [torch.tensor(tokens) for tokens in input_train]  # Convert input train tokens to tensors\n",
    "target_train = [torch.tensor(tokens) for tokens in target_train]  # Convert target train tokens to tensors\n",
    "input_val = [torch.tensor(tokens) for tokens in input_val]  # Convert input validation tokens to tensors\n",
    "target_val = [torch.tensor(tokens) for tokens in target_val]  # Convert target validation tokens to tensors\n",
    "input_test = [torch.tensor(tokens) for tokens in input_test]  # Convert input test tokens to tensors\n",
    "target_test = [torch.tensor(tokens) for tokens in target_test]  # Convert target test tokens to tensors\n",
    "\n",
    "train_dataset = list(zip(input_train, target_train))  # Create a list of train data samples\n",
    "val_dataset = list(zip(input_val, target_val))  # Create a list of validation data samples\n",
    "test_dataset = list(zip(input_test, target_test))  # Create a list of test data samples\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)  # Create a data loader for train data\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)  # Create a data loader for validation data\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)  # Create a data loader for test data\n",
    "\n",
    "model = ThinkTAI(len(sp), len(sp), pretrained_weights=MODEL_PATH) if os.path.isfile(MODEL_PATH) else ThinkTAI(len(sp), len(sp))  # Initialize the ThinkTAI model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Choose the device for training (GPU if available, else CPU)\n",
    "model.to(device)  # Move the model to the device\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())  # Initialize the optimizer\n",
    "lr_scheduler = ReduceLROnPlateau(optimizer, patience=3)  # Learning rate scheduler\n",
    "\n",
    "writer = SummaryWriter(log_dir=\"logs\")  # Initialize the SummaryWriter for TensorBoard logging\n",
    "\n",
    "NUM_EPOCHS = 10  # Number of training epochs\n",
    "best_val_loss = float('inf')  # Initialize the best validation loss\n",
    "early_stop_counter = 0  # Counter for early stopping\n",
    "early_stop_patience = 5  # Patience for early stopping\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()  # Set the model to training mode\n",
    "    train_loss = 0.0  # Initialize the train loss\n",
    "    for src, tgt in train_loader:\n",
    "        src = src.to(device)  # Move the source sequences to the device\n",
    "        tgt = tgt.to(device)  # Move the target sequences to the device\n",
    "\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        output = model(src, tgt[:, :-1])  # Forward pass through the model\n",
    "        loss = F.cross_entropy(output.view(-1, output.shape[-1]), tgt[:, 1:].contiguous().view(-1))  # Compute the loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update the parameters\n",
    "\n",
    "        train_loss += loss.item()  # Accumulate the train loss\n",
    "\n",
    "    train_loss /= len(train_loader)  # Compute the average train loss\n",
    "\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0  # Initialize the validation loss\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in val_loader:\n",
    "            src = src.to(device)  # Move the source sequences to the device\n",
    "            tgt = tgt.to(device)  # Move the target sequences to the device\n",
    "\n",
    "            output = model(src, tgt[:, :-1])  # Forward pass through the model\n",
    "            loss = F.cross_entropy(output.view(-1, output.shape[-1]), tgt[:, 1:].contiguous().view(-1))  # Compute the loss\n",
    "            val_loss += loss.item()  # Accumulate the validation loss\n",
    "\n",
    "        val_loss /= len(val_loader)  # Compute the average validation loss\n",
    "\n",
    "        writer.add_scalar(\"Loss/Train\", train_loss, epoch)  # Write the train loss to TensorBoard\n",
    "        writer.add_scalar(\"Loss/Validation\", val_loss, epoch)  # Write the validation loss to TensorBoard\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss  # Update the best validation loss\n",
    "            early_stop_counter = 0  # Reset the early stop counter\n",
    "            torch.save(model.state_dict(), MODEL_PATH)  # Save the model weights\n",
    "        else:\n",
    "            early_stop_counter += 1  # Increment the early stop counter\n",
    "\n",
    "        if early_stop_counter >= early_stop_patience:\n",
    "            break  # Perform early stopping if the counter exceeds the patience\n",
    "\n",
    "    lr_scheduler.step(val_loss)  # Update the learning rate based on the validation loss\n",
    "\n",
    "    print(f\"Epoch: {epoch + 1}/{NUM_EPOCHS} | Train Loss: {train_loss:.3f} | Val Loss: {val_loss:.3f}\")\n",
    "\n",
    "model.load_state_dict(torch.load(MODEL_PATH))  # Load the best model weights\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "test_loss = 0.0  # Initialize the test loss\n",
    "with torch.no_grad():\n",
    "    for src, tgt in test_loader:\n",
    "        src = src.to(device)  # Move the source sequences to the device\n",
    "        tgt = tgt.to(device)  # Move the target sequences to the device\n",
    "\n",
    "        output = model(src, tgt[:, :-1])  # Forward pass through the model\n",
    "        loss = F.cross_entropy(output.view(-1, output.shape[-1]), tgt[:, 1:].contiguous().view(-1))  # Compute the loss\n",
    "        test_loss += loss.item()  # Accumulate the test loss\n",
    "\n",
    "test_loss /= len(test_loader)  # Compute the average test loss\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.3f}\")  # Print the test loss\n",
    "\n",
    "# Example usage\n",
    "input_sequence = \"Hello, how are you?\"  # Example input sequence\n",
    "input_tokens = tokenize_sentence(input_sequence)  # Tokenize the input sequence\n",
    "input_tokens = torch.LongTensor(input_tokens).unsqueeze(0).to(device)  # Convert the input tokens to a tensor and move to the device\n",
    "\n",
    "output_tokens = inference(model, input_tokens)  # Perform inference using the model\n",
    "output_sequence = sp.DecodeIds(output_tokens)  # Decode the output tokens using SentencePiece tokenizer\n",
    "print(output_sequence)  # Print the predicted output sequence\n",
    "\n",
    "writer.close()  # Close the SummaryWriter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
