{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 123s 3s/step - loss: 0.8369 - val_loss: 0.2617\n",
      "1/1 [==============================] - 1s 910ms/step\n",
      "1/1 [==============================] - 1s 662ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "Predicted Subject:  580 580 580 fe fe fe x6 x6 leaf boss boss boss leaf boss boss boss boss boss boss boss boss boss boss\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Bidirectional, Input, Concatenate, Layer\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Global configuration variables\n",
    "DATA_PATH = r\"./Data/data.txt\"  # Path to the data file\n",
    "CHECKPOINT_PATH = r\"./Model/model_checkpoint.h5\"  # Path to save model checkpoints\n",
    "MAX_SEQ_LENGTH = 100  # Maximum sequence length for tokenization and padding\n",
    "EMBEDDING_DIM = 256  # Dimension of the word embeddings\n",
    "NUM_EPOCHS = 1000  # Number of training epochs\n",
    "BATCH_SIZE = 2  # Size of the mini-batches for training\n",
    "TEST_SPLIT_SIZE = 0.2  # Fraction of the data to use as validation data\n",
    "RANDOM_STATE = 42  # Seed for random number generator for reproducibility\n",
    "EARLY_STOPPING_PATIENCE = 10  # Number of epochs with no improvement after which training will be stopped\n",
    "LSTM_UNITS = 256  # Number of units in LSTM layer\n",
    "LOG_DIR = './Logs'  # Directory to save TensorBoard logs\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, units):\n",
    "        super(Attention, self).__init__()\n",
    "        self.W1 = Dense(units)\n",
    "        self.W2 = Dense(units)\n",
    "        self.V = Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "def load_and_preprocess_data(data_file):\n",
    "    if not os.path.exists(data_file):\n",
    "        raise FileNotFoundError(f\"The file {data_file} does not exist.\")\n",
    "    \n",
    "    data = []\n",
    "    with open(data_file, 'r') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                description, subject = line.strip().split(\"|\")\n",
    "                data.append((description, subject))\n",
    "            except ValueError:\n",
    "                print(f\"Skipped line due to wrong format: {line}\")\n",
    "\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    return [sample[0] for sample in data], [sample[1] for sample in data]\n",
    "\n",
    "def tokenize_and_pad(texts, max_seq_length):\n",
    "    tokenizer = Tokenizer(num_words=None, oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_seq_length, padding='post')\n",
    "\n",
    "    return tokenizer, padded_sequences\n",
    "\n",
    "def build_model(max_seq_length, vocab_size_input, vocab_size_output, embedding_dim):\n",
    "    encoder_input = Input(shape=(max_seq_length,))\n",
    "    encoder_embedding_layer = Embedding(vocab_size_input, embedding_dim)\n",
    "    encoder_embedding = encoder_embedding_layer(encoder_input)\n",
    "    encoder_lstm = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True, return_state=True))\n",
    "    encoder_output, forward_h, forward_c, backward_h, backward_c = encoder_lstm(encoder_embedding)\n",
    "    state_h = Concatenate()([forward_h, backward_h])\n",
    "    state_c = Concatenate()([forward_c, backward_c])\n",
    "\n",
    "    decoder_input = Input(shape=(max_seq_length,))\n",
    "    decoder_embedding_layer = Embedding(vocab_size_output, embedding_dim)\n",
    "    decoder_embedding = decoder_embedding_layer(decoder_input)\n",
    "    decoder_lstm = LSTM(LSTM_UNITS*2, return_sequences=True, return_state=True)\n",
    "\n",
    "    attention_layer = Attention(LSTM_UNITS*2)\n",
    "    context_vector, attention_weights = attention_layer(encoder_output, state_h)\n",
    "    decoder_concat_input = Concatenate(axis=-1)([decoder_embedding, tf.repeat(tf.expand_dims(context_vector, 1), repeats=MAX_SEQ_LENGTH, axis=1)])\n",
    "\n",
    "    decoder_output, _, _ = decoder_lstm(decoder_concat_input, initial_state=[state_h, state_c])\n",
    "    decoder_output = Dense(vocab_size_output, activation='softmax')(decoder_output)\n",
    "\n",
    "    model = Model(inputs=[encoder_input, decoder_input], outputs=decoder_output)\n",
    "\n",
    "    return model, encoder_input, encoder_output, state_h, state_c, decoder_input, decoder_embedding_layer, decoder_lstm, attention_layer\n",
    "\n",
    "descriptions, subjects = load_and_preprocess_data(DATA_PATH)\n",
    "\n",
    "tokenizer_input, sequences_input = tokenize_and_pad(descriptions, MAX_SEQ_LENGTH)\n",
    "tokenizer_output, sequences_output = tokenize_and_pad(subjects, MAX_SEQ_LENGTH)\n",
    "\n",
    "vocab_size_input = len(tokenizer_input.word_index) + 1\n",
    "vocab_size_output = len(tokenizer_output.word_index) + 1\n",
    "model, encoder_input, encoder_output, state_h, state_c, decoder_input, decoder_embedding_layer, decoder_lstm, attention_layer = build_model(\n",
    "    MAX_SEQ_LENGTH, vocab_size_input, vocab_size_output, EMBEDDING_DIM)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "sequences_input_train, sequences_input_val, sequences_output_train, sequences_output_val = train_test_split(\n",
    "    sequences_input, np.expand_dims(sequences_output, -1), test_size=TEST_SPLIT_SIZE, random_state=RANDOM_STATE)\n",
    "\n",
    "if not os.path.exists(os.path.dirname(CHECKPOINT_PATH)):\n",
    "    os.makedirs(os.path.dirname(CHECKPOINT_PATH))\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(CHECKPOINT_PATH, save_weights_only=True, save_best_only=True, monitor='val_loss')\n",
    "tensorboard_callback = TensorBoard(log_dir=LOG_DIR)\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=EARLY_STOPPING_PATIENCE)\n",
    "\n",
    "model.fit([sequences_input_train, sequences_output_train], sequences_output_train,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=NUM_EPOCHS,\n",
    "          validation_data=([sequences_input_val, sequences_output_val], sequences_output_val),\n",
    "          callbacks=[checkpoint_callback, tensorboard_callback, early_stopping_callback])\n",
    "\n",
    "encoder_model = Model(encoder_input, [encoder_output, state_h, state_c])\n",
    "\n",
    "decoder_state_input_h = Input(shape=(LSTM_UNITS*2,))\n",
    "decoder_state_input_c = Input(shape=(LSTM_UNITS*2,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_hidden_state_input = Input(shape=(MAX_SEQ_LENGTH, LSTM_UNITS*2))\n",
    "\n",
    "decoder_input_inf = Input(shape=(1,))\n",
    "decoder_embedding_inf = decoder_embedding_layer(decoder_input_inf)\n",
    "\n",
    "context_vector, _ = attention_layer(decoder_hidden_state_input, decoder_state_input_h)\n",
    "decoder_concat_input = Concatenate(axis=-1)([decoder_embedding_inf, tf.expand_dims(context_vector, 1)])\n",
    "\n",
    "decoder_outputs_inf, state_h_inf, state_c_inf = decoder_lstm(decoder_concat_input, initial_state=decoder_states_inputs)\n",
    "decoder_states_inf = [state_h_inf, state_c_inf]\n",
    "decoder_outputs_inf = Dense(vocab_size_output, activation='softmax')(decoder_outputs_inf)\n",
    "decoder_model = Model([decoder_input_inf] + decoder_states_inputs + [decoder_hidden_state_input], [decoder_outputs_inf] + decoder_states_inf)\n",
    "\n",
    "int_to_word_decoder = {i: word for word, i in tokenizer_output.word_index.items()}\n",
    "int_to_word_decoder[1] = '<OOV>'\n",
    "\n",
    "def decode_sequence(input_sequence):\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_sequence)\n",
    "    states_value = [e_h, e_c]\n",
    "    target_sequence = np.zeros((1, 1))\n",
    "    target_sequence[0, 0] = 1\n",
    "    output_sequence = ''\n",
    "    while True:\n",
    "        output_tokens, h, c = decoder_model.predict([target_sequence] + states_value + [e_out])\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_word = int_to_word_decoder.get(sampled_token_index, '<OOV>')  # dealing with the OOV token\n",
    "        output_sequence += ' ' + sampled_word\n",
    "        if sampled_word == '<end>' or len(output_sequence) > MAX_SEQ_LENGTH:\n",
    "            break\n",
    "        target_sequence = np.zeros((1, 1))\n",
    "        target_sequence[0, 0] = sampled_token_index\n",
    "        states_value = [h, c]\n",
    "    return output_sequence\n",
    "\n",
    "while True:\n",
    "    input_description = input(\"Enter a new description (type 'quit' to exit): \")\n",
    "    if input_description.lower() == 'quit':\n",
    "        break\n",
    "    input_sequence = tokenizer_input.texts_to_sequences([input_description])\n",
    "    input_sequence = pad_sequences(input_sequence, maxlen=MAX_SEQ_LENGTH, padding='post')\n",
    "    predicted_sequence = decode_sequence(input_sequence)\n",
    "    print(\"Predicted Subject:\", predicted_sequence)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
