{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Related third-party imports\n",
    "from IPython.display import clear_output\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import numpy as np\n",
    "import pygame\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.layers import Bidirectional, Concatenate, Dense, Embedding, Input, Layer, LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Defining paths to data file and model checkpoints\n",
    "DATA_PATH = './Data/data.txt'\n",
    "ENCODER_PATH = './Model/encoder_weights.h5'\n",
    "DECODER_PATH = './Model/decoder_weights.h5'\n",
    "\n",
    "# Setting hyperparameters for the model\n",
    "MAX_SEQ_LENGTH = 100\n",
    "EMBEDDING_DIM = 1024\n",
    "NUM_EPOCHS = 1000\n",
    "BATCH_SIZE = 16\n",
    "TEST_SPLIT_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "EARLY_STOPPING_PATIENCE = 5\n",
    "LSTM_UNITS = 1024\n",
    "LOG_DIR = './Logs'\n",
    "\n",
    "# Define the number of records for BLEU testing\n",
    "BLEU_TEST_COUNT = 1\n",
    "\n",
    "# Define training device 'CPU'/'GPU' \n",
    "DEVICE = 'CPU'\n",
    "\n",
    "# Defining Attention layer as a custom Keras layer\n",
    "class Attention(Layer):\n",
    "    def __init__(self, units):\n",
    "        super(Attention, self).__init__()\n",
    "        # Dense layers for calculating attention scores\n",
    "        self.W1 = Dense(units)\n",
    "        self.W2 = Dense(units)\n",
    "        self.V = Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        # Expanding the hidden state dimension for addition operation with features\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        # Calculating attention scores\n",
    "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "# Defining a custom Keras callback for saving model weights\n",
    "class ModelCheckpoint(Callback):\n",
    "    def __init__(self, encoder_model, decoder_model):\n",
    "        super().__init__()\n",
    "        self.encoder_model = encoder_model\n",
    "        self.decoder_model = decoder_model\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current_val_loss = logs.get('val_loss')\n",
    "        # Save the model if it's the first epoch or if the validation loss has improved\n",
    "        if epoch == 0 or current_val_loss < self.best_val_loss: \n",
    "            self.best_val_loss = current_val_loss\n",
    "            self.encoder_model.save_weights(ENCODER_PATH)\n",
    "            self.decoder_model.save_weights(DECODER_PATH)\n",
    "\n",
    "class BleuScoreCallback(Callback):\n",
    "    def __init__(self, tokenizer_output, sequences_input_val, sequences_output_val, num_records):\n",
    "        super().__init__()\n",
    "        # Store the tokenizer for output sequences\n",
    "        self.tokenizer_output = tokenizer_output  \n",
    "        # Store the input validation sequences\n",
    "        self.sequences_input_val = sequences_input_val  \n",
    "        # Store the output validation sequences\n",
    "        self.sequences_output_val = sequences_output_val  \n",
    "        # Store the number of records to select for BLEU score calculation\n",
    "        self.num_records = num_records  \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Create a mapping from integer tokens to words\n",
    "        int_to_word_decoder = {i: word for word, i in self.tokenizer_output.word_index.items()}  \n",
    "        # Set the mapping for the out-of-vocabulary token\n",
    "        int_to_word_decoder[1] = '<OOV>'  \n",
    "        # Initialize a list to store the reference sequences\n",
    "        references = []  \n",
    "        # Initialize a list to store the predicted sequences\n",
    "        candidates = []  \n",
    "        # Randomly select indices for the desired number of records\n",
    "        selected_indices = random.sample(range(len(self.sequences_input_val)), self.num_records)  \n",
    "        # Select the input validation sequences based on the selected indices\n",
    "        selected_sequences_input_val = self.sequences_input_val[selected_indices]\n",
    "        # Select the output validation sequences based on the selected indices  \n",
    "        selected_sequences_output_val = self.sequences_output_val[selected_indices]  \n",
    "        # Get the total number of selected sequences\n",
    "        total_sequences = len(selected_indices) \n",
    "        # Initialize a counter for processed sequences and average for bleu score calculations\n",
    "        processed_sequences = 0  \n",
    "        bleu_score_average = 0\n",
    "        # Record the start time for elapsed time calculation\n",
    "        start_time = time.time() \n",
    "        # Print new line to help with BLEU score formatting \n",
    "        print()\n",
    "        # BLEU score testing and calculation loop\n",
    "        for seq_in, seq_out in zip(selected_sequences_input_val, selected_sequences_output_val):\n",
    "            # Decode the output sequence\n",
    "            predicted_sequence = decode_sequence(np.array([seq_in]))\n",
    "            # Convert the output sequence indices to words\n",
    "            reference_sequence = ' '.join([int_to_word_decoder[int(i)] for i in seq_out if i > 0])  \n",
    "            # Strip '<start>' and '<end>' tokens if they exist\n",
    "            predicted_sequence = predicted_sequence.replace('<start>', '').replace('<end>', '').strip()\n",
    "            reference_sequence = reference_sequence.replace('<start>', '').replace('<end>', '').strip()\n",
    "            # Append the reference sequence as a list of words\n",
    "            references.append([reference_sequence.split()])  \n",
    "            # Append the predicted sequence as a list of words\n",
    "            candidates.append(predicted_sequence.split())  \n",
    "            # Increment the counter for processed sequences\n",
    "            processed_sequences += 1  \n",
    "            # Calculate the progress percentage\n",
    "            progress = processed_sequences / total_sequences * 100  \n",
    "            # Calculate the elapsed time\n",
    "            elapsed_time = time.time() - start_time  \n",
    "            # Calculate the BLEU score\n",
    "            bleu_score_average = (corpus_bleu(references, candidates) +  bleu_score_average) / processed_sequences \n",
    "            # Print progress with carriage return to overwrite the previous line\n",
    "            sys.stdout.write(f'\\rProcessing sequences: {processed_sequences}/{total_sequences} ({progress:.2f}%), Elapsed Time: {elapsed_time:.2f}s, BLEU-1 Average: {bleu_score_average}')\n",
    "            sys.stdout.flush()\n",
    "        # Print new line to help with BLEU score formatting \n",
    "        print()\n",
    "            \n",
    "# Function for sending an audible ping to the user\n",
    "def play_ping():\n",
    "    pygame.mixer.init()\n",
    "    pygame.mixer.music.load(\"./Sounds/notification_by_UNIVERSFIELD.mp3\")  # replace with the path to your sound file\n",
    "    pygame.mixer.music.play()\n",
    "\n",
    "# Function for prompting a user for input, notifying the user with an audible ping, and accepting and returning the input value\n",
    "def input_with_notification(prompt):\n",
    "    # Plays ping letting user know input is requested\n",
    "    play_ping()\n",
    "    # Prints prompt to screen before requesting user input\n",
    "    user_input = input(prompt)\n",
    "    # Returns input value\n",
    "    return user_input\n",
    "\n",
    "# Function for loading and preprocessing the data\n",
    "def load_and_preprocess_data(data_file):\n",
    "    # Raise an error if the data file does not exist\n",
    "    if not os.path.exists(data_file):\n",
    "        raise FileNotFoundError(f'The file {data_file} does not exist.')\n",
    "    data = []\n",
    "    # Open the data file and read lines\n",
    "    with open(data_file, 'r') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                # Split each line into description and subject and append to the data list\n",
    "                description, subject = line.strip().split('|')\n",
    "                data.append((description, subject))\n",
    "            except ValueError:\n",
    "                print(f'Skipped line due to wrong format: {line}')\n",
    "    # Shuffle the data for randomness\n",
    "    np.random.shuffle(data)\n",
    "    # Return the descriptions and subjects as separate lists\n",
    "    return [sample[0] for sample in data], [sample[1] for sample in data]\n",
    "\n",
    "# Function for tokenizing and padding the text data\n",
    "def tokenize_and_pad(texts, max_seq_length):\n",
    "    # Initialize a tokenizer with a special out-of-vocabulary token\n",
    "    tokenizer = Tokenizer(num_words=None, oov_token='<OOV>')\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    # Convert texts to sequences of integer tokens\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    # Pad the sequences to the maximum sequence length\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_seq_length, padding='post')\n",
    "    return tokenizer, padded_sequences\n",
    "\n",
    "# Function for building the model\n",
    "def build_model(max_seq_length, vocab_size_input, vocab_size_output, embedding_dim):\n",
    "    # Define the encoder input layer\n",
    "    encoder_input = Input(shape=(max_seq_length,))\n",
    "    encoder_embedding_layer = Embedding(vocab_size_input, embedding_dim)\n",
    "    encoder_embedding = encoder_embedding_layer(encoder_input)\n",
    "    # Define the encoder LSTM layer\n",
    "    encoder_lstm = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True, return_state=True))\n",
    "    encoder_output, forward_h, forward_c, backward_h, backward_c = encoder_lstm(encoder_embedding)\n",
    "    # Concatenate the forward and backward hidden states\n",
    "    state_h = Concatenate()([forward_h, backward_h])\n",
    "    state_c = Concatenate()([forward_c, backward_c])\n",
    "    # Define the decoder input layer\n",
    "    decoder_input = Input(shape=(max_seq_length,))\n",
    "    decoder_embedding_layer = Embedding(vocab_size_output, embedding_dim)\n",
    "    decoder_embedding = decoder_embedding_layer(decoder_input)\n",
    "    # Define the decoder LSTM layer\n",
    "    decoder_lstm = LSTM(LSTM_UNITS*2, return_sequences=True, return_state=True)\n",
    "    # Define the attention layer\n",
    "    attention_layer = Attention(LSTM_UNITS*2)\n",
    "    context_vector, attention_weights = attention_layer(encoder_output, state_h)\n",
    "    decoder_concat_input = Concatenate(axis=-1)([decoder_embedding, tf.repeat(tf.expand_dims(context_vector, 1), repeats=MAX_SEQ_LENGTH, axis=1)])\n",
    "    # Pass the concatenated input to the decoder LSTM\n",
    "    decoder_output, _, _ = decoder_lstm(decoder_concat_input, initial_state=[state_h, state_c])\n",
    "    # Define the output layer\n",
    "    decoder_output = Dense(vocab_size_output, activation='softmax')(decoder_output)\n",
    "    # Define the model with the encoder and decoder inputs and the decoder output\n",
    "    model = Model(inputs=[encoder_input, decoder_input], outputs=decoder_output)\n",
    "    return model, encoder_input, encoder_output, state_h, state_c, decoder_input, decoder_embedding_layer, decoder_lstm, attention_layer\n",
    "\n",
    "# Function for decoding a sequence of tokens into text\n",
    "def decode_sequence(input_sequence):\n",
    "    # Pass the input sequence to the encoder model and get the output and states\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_sequence, verbose=0)\n",
    "    states_value = [e_h, e_c]\n",
    "    # Initialize the target sequence with the start token\n",
    "    target_sequence = np.zeros((1, 1))\n",
    "    target_sequence[0, 0] = 1\n",
    "    output_sequence = ''\n",
    "    while True:\n",
    "        # Pass the target sequence and states to the decoder model and get the output tokens and new states\n",
    "        output_tokens, h, c = decoder_model.predict([target_sequence] + states_value + [e_out], verbose=0)\n",
    "        # Get the token with the highest probability\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        # Convert the token into a word\n",
    "        sampled_word = int_to_word_decoder.get(sampled_token_index, '<OOV>')  # dealing with the OOV token\n",
    "        output_sequence += ' ' + sampled_word\n",
    "        # Stop if the end token is found or the maximum sequence length is reached\n",
    "        if sampled_word == '<end>' or len(output_sequence) > MAX_SEQ_LENGTH:\n",
    "            break\n",
    "        # Update the target sequence and states for the next iteration\n",
    "        target_sequence = np.zeros((1, 1))\n",
    "        target_sequence[0, 0] = sampled_token_index\n",
    "        states_value = [h, c]\n",
    "    return output_sequence\n",
    "\n",
    "# Refresh all outputs before we begin\n",
    "clear_output()\n",
    "\n",
    "# Configure tensorflow to use defined device for training\n",
    "tf.config.set_visible_devices(tf.config.list_physical_devices(DEVICE)[0], DEVICE)\n",
    "\n",
    "# Load and preprocess the data\n",
    "descriptions, subjects = load_and_preprocess_data(DATA_PATH)\n",
    "\n",
    "# Tokenize and pad the descriptions and subjects\n",
    "tokenizer_input, sequences_input = tokenize_and_pad(descriptions, MAX_SEQ_LENGTH)\n",
    "tokenizer_output, sequences_output = tokenize_and_pad(subjects, MAX_SEQ_LENGTH)\n",
    "\n",
    "# Get the vocab sizes for the input and output\n",
    "vocab_size_input = len(tokenizer_input.word_index) + 1\n",
    "vocab_size_output = len(tokenizer_output.word_index) + 1\n",
    "\n",
    "# Create a dictionary for converting integer tokens back into words\n",
    "int_to_word_decoder = {i: word for word, i in tokenizer_output.word_index.items()}\n",
    "int_to_word_decoder[1] = '<OOV>'\n",
    "\n",
    "# Build the model\n",
    "model, encoder_input, encoder_output, state_h, state_c, decoder_input, decoder_embedding_layer, decoder_lstm, attention_layer = build_model(\n",
    "    MAX_SEQ_LENGTH, vocab_size_input, vocab_size_output, EMBEDDING_DIM)\n",
    "\n",
    "# Compile the model with a loss function, optimizer, and metrics\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Split the sequences into training and validation sets\n",
    "sequences_input_train, sequences_input_val, sequences_output_train, sequences_output_val = train_test_split(\n",
    "    sequences_input, np.expand_dims(sequences_output, -1), test_size=TEST_SPLIT_SIZE, random_state=RANDOM_STATE)\n",
    "\n",
    "# Create directories for saving model weights if they do not exist\n",
    "if not os.path.exists(os.path.dirname(ENCODER_PATH)):\n",
    "    os.makedirs(os.path.dirname(ENCODER_PATH))\n",
    "if not os.path.exists(os.path.dirname(DECODER_PATH)):\n",
    "    os.makedirs(os.path.dirname(DECODER_PATH))\n",
    "\n",
    "# Define TensorBoard and early stopping callbacks\n",
    "tensorboard_callback = TensorBoard(log_dir=LOG_DIR)\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=EARLY_STOPPING_PATIENCE)\n",
    "\n",
    "# Ask the user whether to train a new model, load an existing model, or continue training from an existing model\n",
    "user_input = input_with_notification(\"Enter '1' to Train New Model, '2' to Continue Training from Existing Model, or '3' to Load Existing Model for Testing Only: \")\n",
    "\n",
    "# Define the encoder and decoder models for inference\n",
    "encoder_model = Model(encoder_input, [encoder_output, state_h, state_c])\n",
    "decoder_state_input_h = Input(shape=(LSTM_UNITS*2,))\n",
    "decoder_state_input_c = Input(shape=(LSTM_UNITS*2,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_hidden_state_input = Input(shape=(MAX_SEQ_LENGTH, LSTM_UNITS*2))\n",
    "decoder_input_inf = Input(shape=(1,))\n",
    "decoder_embedding_inf = decoder_embedding_layer(decoder_input_inf)\n",
    "context_vector, _ = attention_layer(decoder_hidden_state_input, decoder_state_input_h)\n",
    "decoder_concat_input = Concatenate(axis=-1)([decoder_embedding_inf, tf.expand_dims(context_vector, 1)])\n",
    "decoder_outputs_inf, state_h_inf, state_c_inf = decoder_lstm(decoder_concat_input, initial_state=decoder_states_inputs)\n",
    "decoder_states_inf = [state_h_inf, state_c_inf]\n",
    "decoder_outputs_inf = Dense(vocab_size_output, activation='softmax')(decoder_outputs_inf)\n",
    "decoder_model = Model([decoder_input_inf] + decoder_states_inputs + [decoder_hidden_state_input], [decoder_outputs_inf] + decoder_states_inf)\n",
    "\n",
    "# Define the model checkpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(encoder_model, decoder_model)\n",
    "\n",
    "# If the user chose to train a new model, fit the model and save the best weights\n",
    "if user_input == '1':\n",
    "    # Create a BLEU score callback object with specified parameters\n",
    "    bleu_score_callback = BleuScoreCallback(tokenizer_output, sequences_input_val, sequences_output_val, BLEU_TEST_COUNT)\n",
    "    # Begin training\n",
    "    model.fit([sequences_input_train, sequences_output_train], sequences_output_train,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            epochs=NUM_EPOCHS,\n",
    "            validation_data=([sequences_input_val, sequences_output_val], sequences_output_val),\n",
    "            callbacks=[checkpoint_callback, tensorboard_callback, bleu_score_callback, early_stopping_callback])\n",
    "\n",
    "# If the user chose to continue training from an existing model, load the saved weights and continue training\n",
    "# Please be aware that this requires you to have the exact same architecture as the one that was used when the model was saved\n",
    "elif user_input == '2':\n",
    "    # Load existing weights\n",
    "    encoder_model.load_weights(ENCODER_PATH)\n",
    "    decoder_model.load_weights(DECODER_PATH)\n",
    "    # Create a BLEU score callback object with specified parameters\n",
    "    bleu_score_callback = BleuScoreCallback(tokenizer_output, sequences_input_val, sequences_output_val, BLEU_TEST_COUNT)\n",
    "    # Begin training\n",
    "    model.fit([sequences_input_train, sequences_output_train], sequences_output_train,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            epochs=NUM_EPOCHS,\n",
    "            validation_data=([sequences_input_val, sequences_output_val], sequences_output_val),\n",
    "            callbacks=[checkpoint_callback, tensorboard_callback, bleu_score_callback, early_stopping_callback])\n",
    "\n",
    "# If the user chose to load an existing model for testing, load the saved weights\n",
    "elif user_input == '3':\n",
    "    encoder_model.load_weights(ENCODER_PATH)\n",
    "    decoder_model.load_weights(DECODER_PATH)\n",
    "\n",
    "# Loop for getting user input and predicting the output\n",
    "while True:\n",
    "    # Ask the user for a description\n",
    "    input_description = input_with_notification(\"Enter a new description (type 'quit' to exit): \")\n",
    "    # Break the loop if the user types 'quit'\n",
    "    if input_description.lower() == 'quit': break\n",
    "    # Tokenize and pad the user's description\n",
    "    input_sequence = tokenizer_input.texts_to_sequences([input_description])\n",
    "    input_sequence = pad_sequences(input_sequence, maxlen=MAX_SEQ_LENGTH, padding='post')\n",
    "    # Decode the model's output sequence\n",
    "    predicted_sequence = decode_sequence(input_sequence)\n",
    "    # Print the predicted subject\n",
    "    print('Predicted Subject:', predicted_sequence)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
