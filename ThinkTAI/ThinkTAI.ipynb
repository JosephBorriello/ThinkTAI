{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jjbor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'19XFL1H76PE010333'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 273\u001b[0m\n\u001b[0;32m    271\u001b[0m input_train \u001b[39m=\u001b[39m [[token2int[token] \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m seq] \u001b[39mfor\u001b[39;00m seq \u001b[39min\u001b[39;00m input_train]\n\u001b[0;32m    272\u001b[0m target_train \u001b[39m=\u001b[39m [[token2int[token] \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m seq] \u001b[39mfor\u001b[39;00m seq \u001b[39min\u001b[39;00m target_train]\n\u001b[1;32m--> 273\u001b[0m input_val \u001b[39m=\u001b[39m [[token2int[token] \u001b[39mfor\u001b[39;49;00m token \u001b[39min\u001b[39;49;00m seq] \u001b[39mfor\u001b[39;49;00m seq \u001b[39min\u001b[39;49;00m input_val]\n\u001b[0;32m    274\u001b[0m target_val \u001b[39m=\u001b[39m [[token2int[token] \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m seq] \u001b[39mfor\u001b[39;00m seq \u001b[39min\u001b[39;00m target_val]\n\u001b[0;32m    275\u001b[0m input_test \u001b[39m=\u001b[39m [[token2int[token] \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m seq] \u001b[39mfor\u001b[39;00m seq \u001b[39min\u001b[39;00m input_test]\n",
      "Cell \u001b[1;32mIn[2], line 273\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    271\u001b[0m input_train \u001b[39m=\u001b[39m [[token2int[token] \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m seq] \u001b[39mfor\u001b[39;00m seq \u001b[39min\u001b[39;00m input_train]\n\u001b[0;32m    272\u001b[0m target_train \u001b[39m=\u001b[39m [[token2int[token] \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m seq] \u001b[39mfor\u001b[39;00m seq \u001b[39min\u001b[39;00m target_train]\n\u001b[1;32m--> 273\u001b[0m input_val \u001b[39m=\u001b[39m [[token2int[token] \u001b[39mfor\u001b[39;49;00m token \u001b[39min\u001b[39;49;00m seq] \u001b[39mfor\u001b[39;00m seq \u001b[39min\u001b[39;00m input_val]\n\u001b[0;32m    274\u001b[0m target_val \u001b[39m=\u001b[39m [[token2int[token] \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m seq] \u001b[39mfor\u001b[39;00m seq \u001b[39min\u001b[39;00m target_val]\n\u001b[0;32m    275\u001b[0m input_test \u001b[39m=\u001b[39m [[token2int[token] \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m seq] \u001b[39mfor\u001b[39;00m seq \u001b[39min\u001b[39;00m input_test]\n",
      "Cell \u001b[1;32mIn[2], line 273\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    271\u001b[0m input_train \u001b[39m=\u001b[39m [[token2int[token] \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m seq] \u001b[39mfor\u001b[39;00m seq \u001b[39min\u001b[39;00m input_train]\n\u001b[0;32m    272\u001b[0m target_train \u001b[39m=\u001b[39m [[token2int[token] \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m seq] \u001b[39mfor\u001b[39;00m seq \u001b[39min\u001b[39;00m target_train]\n\u001b[1;32m--> 273\u001b[0m input_val \u001b[39m=\u001b[39m [[token2int[token] \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m seq] \u001b[39mfor\u001b[39;00m seq \u001b[39min\u001b[39;00m input_val]\n\u001b[0;32m    274\u001b[0m target_val \u001b[39m=\u001b[39m [[token2int[token] \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m seq] \u001b[39mfor\u001b[39;00m seq \u001b[39min\u001b[39;00m target_val]\n\u001b[0;32m    275\u001b[0m input_test \u001b[39m=\u001b[39m [[token2int[token] \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m seq] \u001b[39mfor\u001b[39;00m seq \u001b[39min\u001b[39;00m input_test]\n",
      "\u001b[1;31mKeyError\u001b[0m: '19XFL1H76PE010333'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import pandas as pd\n",
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "\n",
    "MODEL_PATH = r\"E:/GitHub/ThinkTAI/ThinkTAI/Model/model_weights.pth\"\n",
    "DATA_FILE = r\"E:/GitHub/ThinkTAI/ThinkTAI/Data/data.xlsx\"\n",
    "CLEANED_DATA_FILE = r\"E:/GitHub/ThinkTAI/ThinkTAI/Data/cleaned_data.csv\"\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "D_MODEL = 128\n",
    "N_HEAD = 4\n",
    "NUM_LAYERS = 32\n",
    "DIM_FEEDFORWARD = 512\n",
    "DROPOUT = 0.1\n",
    "MAX_LEN = 5000\n",
    "BATCH_SIZE = 7\n",
    "\n",
    "class TrainingSet:\n",
    "    def __init__(self, url, subject, html):\n",
    "        self.url = url\n",
    "        self.subject = subject\n",
    "        self.html = html\n",
    "\n",
    "def load_data_from_excel(file_path):\n",
    "    training_data = []\n",
    "    df = pd.read_excel(file_path)\n",
    "    for _, row in df.iterrows():\n",
    "        url = row['URL']\n",
    "        subject = row['Subject']\n",
    "        html = \"\"\n",
    "        training_data.append(TrainingSet(url, subject, html))\n",
    "    return training_data\n",
    "\n",
    "def retrieve_html_content(data):\n",
    "    try:\n",
    "        response = requests.get(data.url)\n",
    "        if response.ok and response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            if soup.find():\n",
    "                data.html = response.content.decode('utf-8')\n",
    "            else:\n",
    "                data.html = \"\"\n",
    "        else:\n",
    "            data.html = \"\"\n",
    "    except requests.RequestException:\n",
    "        data.html = \"\"\n",
    "\n",
    "def remove_empty_html(data_list):\n",
    "    return [data for data in data_list if data.html]\n",
    "\n",
    "def save_cleaned_data_to_csv(file_path, data_list):\n",
    "    cleaned_data = {'URL': [data.url for data in data_list],\n",
    "                    'Subject': [data.subject for data in data_list],\n",
    "                    'HTML': [data.html for data in data_list]}\n",
    "    cleaned_df = pd.DataFrame(cleaned_data)\n",
    "    cleaned_df.to_csv(file_path, index=False)\n",
    "\n",
    "def create_plain_text_file(data_list, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for data in data_list:\n",
    "            if data.html:\n",
    "                writer.writerow([data.html])\n",
    "\n",
    "def load_data_from_csv(file_path):\n",
    "    cleaned_df = pd.read_csv(file_path)\n",
    "    input_data = cleaned_df['HTML'].tolist()\n",
    "    target_data = cleaned_df['Subject'].tolist()\n",
    "    return input_data, target_data\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "def preprocess_data(input_data, target_data):\n",
    "    tokenized_input_data = []\n",
    "    for sentence in input_data:\n",
    "        tokenized_input_data.append(tokenize_sentence(sentence))\n",
    "\n",
    "    tokenized_target_data = []\n",
    "    for sentence in target_data:\n",
    "        tokenized_target_data.append(tokenize_sentence(sentence))\n",
    "    \n",
    "    return tokenized_input_data, tokenized_target_data\n",
    "\n",
    "def tokenize_sentence(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    return tokens\n",
    "\n",
    "def pad_sequence_to_length(sequence, target_length, padding_token):\n",
    "    if len(sequence) < target_length:\n",
    "        pad_length = target_length - len(sequence)\n",
    "        sequence = sequence + [padding_token] * pad_length\n",
    "\n",
    "    return sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_sequences = []\n",
    "    tgt_sequences = []\n",
    "    for src, tgt in batch:\n",
    "        src_sequences.append(src)\n",
    "        tgt_sequences.append(tgt)\n",
    "\n",
    "    max_len = max(len(seq) for seq in src_sequences + tgt_sequences)\n",
    "    src_padded = pad_sequence([torch.tensor(pad_sequence_to_length(seq, max_len, ' ')) for seq in src_sequences],\n",
    "                              batch_first=True)\n",
    "    tgt_padded = pad_sequence([torch.tensor(pad_sequence_to_length(seq, max_len, ' ')) for seq in tgt_sequences],\n",
    "                              batch_first=True)\n",
    "    return src_padded, tgt_padded\n",
    "\n",
    "def inference(model, src, beam_width=5, max_length=100):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    src = src.unsqueeze(0).to(device)  # Add a batch dimension and move to the device\n",
    "    src = src.repeat(beam_width, 1)  # Repeat the source sequence for beam search\n",
    "\n",
    "    with torch.no_grad():\n",
    "        src_encoding = model.embedding(src) * math.sqrt(D_MODEL)  # Embed the source sequence\n",
    "        src_encoding = model.pos_encoder(src_encoding)  # Apply positional encoding to the source sequence\n",
    "        memory = model.transformer_encoder(src_encoding)  # Encode the source sequence\n",
    "\n",
    "        tgt = torch.ones(beam_width, 1).long().to(device)  # Initialize target sequence with start token\n",
    "        tgt_lengths = torch.ones(beam_width).long().to(device)  # Initialize target sequence lengths\n",
    "        eos_flags = torch.zeros(beam_width).byte().to(device)  # Flags to track if beam search paths have reached end-of-sequence\n",
    "\n",
    "        scores_beam = torch.zeros(beam_width).to(device)  # Initialize scores_beam tensor\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            tgt_encoding = model.embedding(tgt) * math.sqrt(D_MODEL)  # Embed the target sequence\n",
    "            tgt_encoding = model.pos_encoder(tgt_encoding)  # Apply positional encoding to the target sequence\n",
    "            output = model.transformer_decoder(tgt_encoding, memory)  # Decode the target sequence\n",
    "\n",
    "            output = model.decoder(output[:, -1, :])  # Get logits for the last token\n",
    "            output = F.log_softmax(output, dim=-1)  # Apply log softmax to convert logits to probabilities\n",
    "\n",
    "            output = output.view(beam_width, -1, OUTPUT_DIM)  # Reshape logits for beam search\n",
    "\n",
    "            if _ == 0:\n",
    "                scores, candidates = output.topk(beam_width, dim=-1)  # Get top-k scores and candidates\n",
    "            else:\n",
    "                scores, candidates = output.topk(beam_width, dim=-1)  # Get top-k scores and candidates\n",
    "                scores = scores + scores_beam.unsqueeze(2)  # Add scores of previous beam search paths\n",
    "\n",
    "            scores = scores.view(beam_width, -1)  # Reshape scores for beam search\n",
    "            candidates = candidates.view(beam_width, -1)  # Reshape candidates for beam search\n",
    "\n",
    "            if _ == 0:\n",
    "                scores_flat = scores.squeeze()  # Flatten scores for beam search\n",
    "            else:\n",
    "                scores_flat = scores.view(-1)  # Flatten scores for beam search\n",
    "\n",
    "            scores_beam, indices_beam = scores_flat.topk(beam_width, dim=-1)  # Get top-k scores and indices\n",
    "\n",
    "            tgt_candidates = candidates.view(-1)  # Flatten candidates for beam search\n",
    "            tgt_candidates_beam = tgt_candidates[indices_beam]  # Select candidates for beam search\n",
    "\n",
    "            tgt = torch.cat((tgt, tgt_candidates_beam.unsqueeze(1)), dim=1)  # Append selected candidates to target sequence\n",
    "\n",
    "            eos_flags = eos_flags | (tgt_candidates_beam == 1)  # Check if any of the selected candidates is the end token\n",
    "            if eos_flags.all():  # Break if all beam search paths have reached end-of-sequence\n",
    "                break\n",
    "\n",
    "            tgt_lengths = tgt_lengths + (~eos_flags).long()  # Update target sequence lengths\n",
    "\n",
    "        best_sequence_index = scores_beam.argmax().item()  # Find the index of the best sequence\n",
    "        best_sequence = tgt[best_sequence_index].tolist()  # Convert the best sequence to a list\n",
    "\n",
    "    return best_sequence[1:]  # Remove the start token from the best sequence\n",
    "\n",
    "# Class definition for the main ThinkTAI model\n",
    "class ThinkTAI(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, pretrained_weights=None):\n",
    "        super(ThinkTAI, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, D_MODEL)  # Embedding layer for input tokens\n",
    "        self.pos_encoder = PositionalEncoding(D_MODEL, DROPOUT, max_len=MAX_LEN)  # Positional encoding layer\n",
    "\n",
    "        encoder_layers = nn.TransformerEncoderLayer(D_MODEL, N_HEAD, DIM_FEEDFORWARD, DROPOUT)  # Encoder layers for the transformer\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, NUM_LAYERS)  # Transformer encoder\n",
    "\n",
    "        decoder_layers = nn.TransformerDecoderLayer(D_MODEL, N_HEAD, DIM_FEEDFORWARD, DROPOUT)  # Decoder layers for the transformer\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layers, NUM_LAYERS)  # Transformer decoder\n",
    "\n",
    "        self.decoder = nn.Linear(D_MODEL, output_dim)  # Linear layer for output prediction\n",
    "\n",
    "        self.init_weights(pretrained_weights)  # Initialize weights of the model\n",
    "\n",
    "    def init_weights(self, pretrained_weights=None):\n",
    "        if pretrained_weights is not None:\n",
    "            self.load_state_dict(torch.load(pretrained_weights))  # Load pretrained weights if available\n",
    "        else:\n",
    "            initrange = 0.1\n",
    "            self.embedding.weight.data.uniform_(-initrange, initrange)  # Initialize embedding weights uniformly\n",
    "            self.decoder.weight.data.uniform_(-initrange, initrange)  # Initialize decoder weights uniformly\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src = self.embedding(src) * math.sqrt(D_MODEL)  # Embed the source sequence\n",
    "        src = self.pos_encoder(src)  # Apply positional encoding to the source sequence\n",
    "        memory = self.transformer_encoder(src)  # Encode the source sequence\n",
    "        tgt = self.embedding(tgt) * math.sqrt(D_MODEL)  # Embed the target sequence\n",
    "        tgt = self.pos_encoder(tgt)  # Apply positional encoding to the target sequence\n",
    "        output = self.transformer_decoder(tgt, memory)  # Decode the target sequence\n",
    "        output = self.decoder(output)  # Predict the output\n",
    "        return output\n",
    "\n",
    "# Load data from Excel file\n",
    "training_data = load_data_from_excel(DATA_FILE)\n",
    "\n",
    "# Retrieve HTML content from URLs\n",
    "for data in training_data:\n",
    "    retrieve_html_content(data)\n",
    "\n",
    "# Remove objects with empty or null HTML\n",
    "training_data = remove_empty_html(training_data)\n",
    "\n",
    "# Save cleaned data to CSV file\n",
    "save_cleaned_data_to_csv(CLEANED_DATA_FILE, training_data)\n",
    "\n",
    "nltk.download('punkt')  # Download the required NLTK resource\n",
    "\n",
    "# Load cleaned CSV file into input and target data\n",
    "input_data, target_data = load_data_from_csv(CLEANED_DATA_FILE)\n",
    "\n",
    "input_data, target_data = preprocess_data(input_data, target_data)\n",
    "\n",
    "input_train, input_val_test, target_train, target_val_test = train_test_split(input_data, target_data, test_size=0.2, random_state=RANDOM_SEED)\n",
    "input_val, input_test, target_val, target_test = train_test_split(input_val_test, target_val_test, test_size=0.5, random_state=RANDOM_SEED)\n",
    "\n",
    "# Build the vocabulary\n",
    "vocab_counter = Counter()\n",
    "for seq in input_train + target_train:\n",
    "    vocab_counter.update(seq)\n",
    "\n",
    "# Create a mapping from tokens to integers\n",
    "vocab = [token for token, count in vocab_counter.items()]\n",
    "\n",
    "# Create a mapping from integers to tokens\n",
    "int2token = {i: token for i, token in enumerate(vocab)}\n",
    "\n",
    "# Create a mapping from tokens to integers\n",
    "token2int = {token: i for i, token in int2token.items()}\n",
    "\n",
    "# Tokenize and convert to integers\n",
    "input_train = [[token2int[token] for token in seq] for seq in input_train]\n",
    "target_train = [[token2int[token] for token in seq] for seq in target_train]\n",
    "input_val = [[token2int[token] for token in seq] for seq in input_val]\n",
    "target_val = [[token2int[token] for token in seq] for seq in target_val]\n",
    "input_test = [[token2int[token] for token in seq] for seq in input_test]\n",
    "target_test = [[token2int[token] for token in seq] for seq in target_test]\n",
    "\n",
    "# Convert to tensors\n",
    "input_train = [torch.LongTensor(seq) for seq in input_train]\n",
    "target_train = [torch.LongTensor(seq) for seq in target_train]\n",
    "input_val = [torch.LongTensor(seq) for seq in input_val]\n",
    "target_val = [torch.LongTensor(seq) for seq in target_val]\n",
    "input_test = [torch.LongTensor(seq) for seq in input_test]\n",
    "target_test = [torch.LongTensor(seq) for seq in target_test]\n",
    "\n",
    "train_dataset = list(zip(input_train, target_train))\n",
    "val_dataset = list(zip(input_val, target_val))\n",
    "test_dataset = list(zip(input_test, target_test))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "OUTPUT_DIM = len(target_train)  # Set OUTPUT_DIM based on the size of the target vocabulary\n",
    "\n",
    "model = ThinkTAI(len(input_train), OUTPUT_DIM, pretrained_weights=MODEL_PATH) if os.path.isfile(MODEL_PATH) else ThinkTAI(len(input_train), OUTPUT_DIM)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "lr_scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)  # Adjust the parameters as needed\n",
    "\n",
    "writer = SummaryWriter(log_dir=\"logs\")\n",
    "\n",
    "NUM_EPOCHS = 1\n",
    "best_val_loss = float('inf')\n",
    "early_stop_counter = 0\n",
    "early_stop_patience = 5\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for src, tgt in train_loader:\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt[:, :-1])\n",
    "        print(\"Output shape:\", output.shape)\n",
    "        print(\"Target shape:\", tgt[:, 1:].contiguous().shape)\n",
    "        loss = F.cross_entropy(output.view(-1, output.shape[-1]), tgt[:, 1:].contiguous().view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in val_loader:\n",
    "            src = src.to(device)\n",
    "            tgt = tgt.to(device)\n",
    "\n",
    "            output = model(src, tgt[:, :-1])\n",
    "            loss = F.cross_entropy(output.view(-1, output.shape[-1]), tgt[:, 1:].contiguous().view(-1))\n",
    "            val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        writer.add_scalar(\"Loss/Train\", train_loss, epoch)\n",
    "        writer.add_scalar(\"Loss/Validation\", val_loss, epoch)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            early_stop_counter = 0\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "\n",
    "        if early_stop_counter >= early_stop_patience:\n",
    "            break\n",
    "\n",
    "    lr_scheduler.step(val_loss)\n",
    "\n",
    "    print(f\"Epoch: {epoch + 1}/{NUM_EPOCHS} | Train Loss: {train_loss:.3f} | Val Loss: {val_loss:.3f}\")\n",
    "\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for src, tgt in test_loader:\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "\n",
    "        output = model(src, tgt[:, :-1])\n",
    "        loss = F.cross_entropy(output.view(-1, output.shape[-1]), tgt[:, 1:].contiguous().view(-1))\n",
    "        test_loss += loss.item()\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.3f}\")\n",
    "\n",
    "input_sequence = \"Hello, how are you?\"\n",
    "input_tokens = tokenize_sentence(input_sequence)\n",
    "input_tokens = torch.LongTensor(input_tokens).unsqueeze(0).to(device)\n",
    "\n",
    "output_tokens = inference(model, input_tokens)\n",
    "\n",
    "output_sequence = \" \".join(output_tokens)  # Convert the output tokens to a string\n",
    "print(output_sequence)\n",
    "\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
