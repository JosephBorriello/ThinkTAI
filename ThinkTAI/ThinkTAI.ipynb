{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Related third-party imports\n",
    "from IPython.display import clear_output\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import pygame\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.layers import Bidirectional, Concatenate, Dense, Embedding, Input, Layer, LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Defining paths to data file and model checkpoints\n",
    "LOG_DIR = './Logs'\n",
    "DATA_PATH = './Data'\n",
    "TOKENIZER_DIR = './Tokenizer'\n",
    "TRAINING_DIR = './Training'\n",
    "MODEL_DIR = './Model'\n",
    "ENCODER_TRAINING_PATH = './Training/encoder_weights.keras'\n",
    "DECODER_TRAINING_PATH = './Training/decoder_weights.keras'\n",
    "ENCODER_PATH = './Model/encoder_weights.keras'\n",
    "DECODER_PATH = './Model/decoder_weights.keras'\n",
    "\n",
    "# Setting hyperparameters for the model\n",
    "MAX_SEQ_LENGTH = 32000\n",
    "EMBEDDING_DIM = 256\n",
    "NUM_EPOCHS = 1000\n",
    "BATCH_SIZE = 8\n",
    "TEST_SPLIT_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "EARLY_STOPPING_PATIENCE = 5\n",
    "LSTM_UNITS = 256\n",
    "\n",
    "# Define the number of records for BLEU testing\n",
    "BLEU_TEST_COUNT = 4\n",
    "\n",
    "# Define training device 'CPU'/'GPU' \n",
    "DEVICE = 'CPU'\n",
    "\n",
    "# Defining Attention layer as a custom Keras layer\n",
    "class Attention(Layer):\n",
    "    def __init__(self, units):\n",
    "        super(Attention, self).__init__()\n",
    "        # Dense layers for calculating attention scores\n",
    "        self.W1 = Dense(units)\n",
    "        self.W2 = Dense(units)\n",
    "        self.V = Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)  # Expand the hidden state dimension for addition operation with features\n",
    "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))  # Calculate attention scores\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)  # Apply softmax to calculate attention weights\n",
    "        context_vector = attention_weights * features  # Calculate the context vector by element-wise multiplication\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)  # Sum the context vector across the time axis\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "# Defining a custom Keras callback for saving model weights\n",
    "class ModelCheckpoint(Callback):\n",
    "    def __init__(self, encoder_model, decoder_model):\n",
    "        super().__init__()\n",
    "        self.encoder_model = encoder_model\n",
    "        self.decoder_model = decoder_model\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        # Save the entire model after each batch\n",
    "        self.encoder_model.save_weights(ENCODER_TRAINING_PATH)\n",
    "        self.decoder_model.save_weights(DECODER_TRAINING_PATH)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current_val_loss = logs.get('val_loss')\n",
    "        # Save the entire model if it's the first epoch or if the validation loss has improved\n",
    "        if epoch == 0 or current_val_loss < self.best_val_loss:\n",
    "            self.best_val_loss = current_val_loss\n",
    "            self.encoder_model.save_weights(ENCODER_PATH)\n",
    "            self.decoder_model.save_weights(DECODER_PATH)\n",
    "\n",
    "class BleuScoreCallback(Callback):\n",
    "    def __init__(self, tokenizer_output, sequences_input_val, sequences_output_val, num_records, log_dir):\n",
    "        super().__init__()\n",
    "        # Store the tokenizer for output sequences\n",
    "        self.tokenizer_output = tokenizer_output  \n",
    "        # Store the input validation sequences\n",
    "        self.sequences_input_val = sequences_input_val  \n",
    "        # Store the output validation sequences\n",
    "        self.sequences_output_val = sequences_output_val  \n",
    "        # Store the number of records to select for BLEU score calculation\n",
    "        self.num_records = num_records  \n",
    "        # TensorBoard logging\n",
    "        self.log_dir = log_dir\n",
    "        self.file_writer = tf.summary.create_file_writer(self.log_dir)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Create a mapping from integer tokens to words\n",
    "        int_to_word_decoder = {i: word for word, i in self.tokenizer_output.word_index.items()}  \n",
    "        # Set the mapping for the out-of-vocabulary token\n",
    "        int_to_word_decoder[1] = '<OOV>'  \n",
    "        # Initialize a list to store the reference sequences\n",
    "        references = []  \n",
    "        # Initialize a list to store the predicted sequences\n",
    "        candidates = []  \n",
    "        # Randomly select indices for the desired number of records\n",
    "        selected_indices = random.sample(range(len(self.sequences_input_val)), self.num_records)  \n",
    "        # Select the input validation sequences based on the selected indices\n",
    "        selected_sequences_input_val = self.sequences_input_val[selected_indices]\n",
    "        # Select the output validation sequences based on the selected indices  \n",
    "        selected_sequences_output_val = self.sequences_output_val[selected_indices]  \n",
    "        # Get the total number of selected sequences\n",
    "        total_sequences = len(selected_indices) \n",
    "        # Initialize a counter for processed sequences and average for bleu score calculations\n",
    "        processed_sequences = 0  \n",
    "        bleu_score_average = 0\n",
    "        # Record the start time for elapsed time calculation\n",
    "        start_time = time.time() \n",
    "        # Print new line to help with BLEU score formatting \n",
    "        print()\n",
    "        # BLEU score testing and calculation loop\n",
    "        for seq_in, seq_out in zip(selected_sequences_input_val, selected_sequences_output_val):\n",
    "            # Decode the output sequence\n",
    "            predicted_sequence = decode_sequence(np.array([seq_in]))\n",
    "            # Convert the output sequence indices to words\n",
    "            reference_sequence = ' '.join([int_to_word_decoder[int(i)] for i in seq_out if i > 0])  \n",
    "            # Strip '<start>' and '<end>' tokens if they exist\n",
    "            predicted_sequence = predicted_sequence.replace('<start>', '').replace('<end>', '').strip()\n",
    "            reference_sequence = reference_sequence.replace('<start>', '').replace('<end>', '').strip()\n",
    "            # Append the reference sequence as a list of words\n",
    "            references.append([reference_sequence.split()])  \n",
    "            # Append the predicted sequence as a list of words\n",
    "            candidates.append(predicted_sequence.split())  \n",
    "            # Increment the counter for processed sequences\n",
    "            processed_sequences += 1  \n",
    "            # Calculate the progress percentage\n",
    "            progress = processed_sequences / total_sequences * 100  \n",
    "            # Calculate the elapsed time\n",
    "            elapsed_time = time.time() - start_time  \n",
    "            # Calculate the BLEU score\n",
    "            bleu_score_average = (corpus_bleu(references, candidates) +  bleu_score_average) / processed_sequences \n",
    "            # Print progress with carriage return to overwrite the previous line\n",
    "            sys.stdout.write(f'\\rProcessing sequences: {processed_sequences}/{total_sequences} ({progress:.2f}%), Elapsed Time: {elapsed_time:.2f}s, BLEU-1 Average: {bleu_score_average}')\n",
    "            sys.stdout.flush()\n",
    "        # Print new line to help with BLEU score formatting \n",
    "        print()\n",
    "        # Tensorboard logging\n",
    "        with self.file_writer.as_default():\n",
    "            tf.summary.scalar('BLEU-1 score', bleu_score_average, step=epoch)\n",
    "            \n",
    "# Function for sending an audible ping to the user\n",
    "def play_ping():\n",
    "    pygame.mixer.init()\n",
    "    pygame.mixer.music.load(\"./Sounds/notification_by_UNIVERSFIELD.mp3\")  # Replace with the path to your sound file\n",
    "    pygame.mixer.music.play()\n",
    "\n",
    "# Function for prompting a user for input, notifying the user with an audible ping, and accepting and returning the input value\n",
    "def input_with_notification(prompt):\n",
    "    # Plays ping letting user know input is requested\n",
    "    play_ping()\n",
    "    # Prints prompt to screen before requesting user input\n",
    "    user_input = input(prompt)\n",
    "    # Returns input value\n",
    "    return user_input\n",
    "\n",
    "# Function to create a directory with a .gitignore \"*\" file\n",
    "def create_directory_with_gitignore(directory_path):\n",
    "    # Check if the directory already exists\n",
    "    if not os.path.exists(directory_path):\n",
    "        # Create the directory\n",
    "        os.makedirs(directory_path)\n",
    "    # Check if .gitignore file exists\n",
    "    gitignore_path = os.path.join(directory_path, '.gitignore')\n",
    "    if not os.path.exists(gitignore_path):\n",
    "        # Create the .gitignore file\n",
    "        with open(gitignore_path, 'w') as gitignore_file:\n",
    "            gitignore_file.write('*')\n",
    "\n",
    "# Function for loading and preprocessing the data\n",
    "def load_and_preprocess_data(DATA_PATH):\n",
    "    if not os.path.exists(DATA_PATH):\n",
    "        raise FileNotFoundError(f'The folder {DATA_PATH} does not exist.')\n",
    "    data = []\n",
    "    for file in os.listdir(DATA_PATH):\n",
    "        file_path = os.path.join(DATA_PATH, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                line_numer = 0\n",
    "                for line in file:\n",
    "                    line_numer = line_numer + 1\n",
    "                    try:\n",
    "                        description, subject = line.strip().rsplit('<HTML+JSON DELIMITER>', 1)\n",
    "                        json_obj = json.loads(subject)\n",
    "                        if 'Image Url List' in json_obj:\n",
    "                            del json_obj['VDP Url']\n",
    "                            del json_obj['Image Url List']\n",
    "                        data.append((description, json.dumps(json_obj)))\n",
    "                    except ValueError:\n",
    "                        print(f'\\nSkipped line in {file.name} due to wrong format. Line Number: {line_numer}')\n",
    "        print(f\"\\rNumber of Training Records Loaded: {len(data)}\", end='', flush=True)\n",
    "    np.random.shuffle(data)\n",
    "    print(\"\\n\")  # Move to the next line after everything is loaded\n",
    "    return [sample[0] for sample in data], [sample[1] for sample in data]\n",
    "\n",
    "# Custom subclass of Tokenizer for progress monitoring\n",
    "class MonitoredTokenizer(Tokenizer):\n",
    "    def fit_on_texts(self, texts):\n",
    "        total_texts = len(texts)\n",
    "        for i, text in enumerate(texts):\n",
    "            # Call the original method for a single text\n",
    "            super(MonitoredTokenizer, self).fit_on_texts([text])        \n",
    "            # Print progress\n",
    "            progress = (i + 1) / total_texts * 100\n",
    "            print(f\"\\rFitting Progress: {progress:.2f}%\", end='', flush=True)\n",
    "        print(\"\\n\")  # Move to the next line after progress\n",
    "\n",
    "# Function for tokenizing and padding the text data\n",
    "def tokenize_and_pad(texts, max_seq_length):\n",
    "    # Initialize a tokenizer with a special out-of-vocabulary token\n",
    "    tokenizer = MonitoredTokenizer(num_words=None, oov_token='<OOV>')\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    # Convert texts to sequences of integer tokens\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    # Initialize counter\n",
    "    counter = 0\n",
    "    total = len(sequences)\n",
    "    # New padded sequences list\n",
    "    padded_sequences = []\n",
    "    # Pad the sequences to the maximum sequence length\n",
    "    for seq in sequences:\n",
    "        # Pad the sequence\n",
    "        padded_seq = pad_sequences([seq], maxlen=max_seq_length, padding='post')[0]\n",
    "        padded_sequences.append(padded_seq)\n",
    "        \n",
    "        # Update counter and print status\n",
    "        counter += 1\n",
    "        print(f\"\\rPadding Progress: {counter}/{total}\", end='', flush=True)\n",
    "    print(\"\\n\")  # Move to the next line after progress\n",
    "\n",
    "    return tokenizer, padded_sequences\n",
    "\n",
    "# Function for building the model\n",
    "def build_model(max_seq_length, vocab_size_input, vocab_size_output, embedding_dim):\n",
    "    # Define the encoder input layer\n",
    "    encoder_input = Input(shape=(max_seq_length,))\n",
    "    encoder_embedding_layer = Embedding(vocab_size_input, embedding_dim)\n",
    "    encoder_embedding = encoder_embedding_layer(encoder_input)\n",
    "    # Define the encoder LSTM layer\n",
    "    encoder_lstm = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True, return_state=True))\n",
    "    encoder_output, forward_h, forward_c, backward_h, backward_c = encoder_lstm(encoder_embedding)\n",
    "    # Concatenate the forward and backward hidden states\n",
    "    state_h = Concatenate()([forward_h, backward_h])\n",
    "    state_c = Concatenate()([forward_c, backward_c])\n",
    "    # Define the decoder input layer\n",
    "    decoder_input = Input(shape=(max_seq_length,))\n",
    "    decoder_embedding_layer = Embedding(vocab_size_output, embedding_dim)\n",
    "    decoder_embedding = decoder_embedding_layer(decoder_input)\n",
    "    # Define the decoder LSTM layer\n",
    "    decoder_lstm = LSTM(LSTM_UNITS*2, return_sequences=True, return_state=True)\n",
    "    # Define the attention layer\n",
    "    attention_layer = Attention(LSTM_UNITS*2)\n",
    "    context_vector, attention_weights = attention_layer(encoder_output, state_h)\n",
    "    decoder_concat_input = Concatenate(axis=-1)([decoder_embedding, tf.repeat(tf.expand_dims(context_vector, 1), repeats=MAX_SEQ_LENGTH, axis=1)])\n",
    "    # Pass the concatenated input to the decoder LSTM\n",
    "    decoder_output, _, _ = decoder_lstm(decoder_concat_input, initial_state=[state_h, state_c])\n",
    "    # Define the output layer\n",
    "    decoder_output = Dense(vocab_size_output, activation='softmax')(decoder_output)\n",
    "    # Define the model with the encoder and decoder inputs and the decoder output\n",
    "    model = Model(inputs=[encoder_input, decoder_input], outputs=decoder_output)\n",
    "    return model, encoder_input, encoder_output, state_h, state_c, decoder_input, decoder_embedding_layer, decoder_lstm, attention_layer\n",
    "\n",
    "# Function for decoding a sequence of tokens into text\n",
    "def decode_sequence(input_sequence):\n",
    "    # Pass the input sequence to the encoder model and get the output and states\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_sequence, verbose=0)\n",
    "    states_value = [e_h, e_c]\n",
    "    # Initialize the target sequence with the start token\n",
    "    target_sequence = np.zeros((1, 1))\n",
    "    target_sequence[0, 0] = 1\n",
    "    output_sequence = ''\n",
    "    while True:\n",
    "        # Pass the target sequence and states to the decoder model and get the output tokens and new states\n",
    "        output_tokens, h, c = decoder_model.predict([target_sequence] + states_value + [e_out], verbose=0)\n",
    "        # Get the token with the highest probability\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        # Convert the token into a word\n",
    "        sampled_word = int_to_word_decoder.get(sampled_token_index, '<OOV>')  # Dealing with the OOV token\n",
    "        output_sequence += ' ' + sampled_word\n",
    "        # Stop if the end token is found or the maximum sequence length is reached\n",
    "        if sampled_word == '<end>' or len(output_sequence) > MAX_SEQ_LENGTH:\n",
    "            break\n",
    "        # Update the target sequence and states for the next iteration\n",
    "        target_sequence = np.zeros((1, 1))\n",
    "        target_sequence[0, 0] = sampled_token_index\n",
    "        states_value = [h, c]\n",
    "    return output_sequence\n",
    "\n",
    "# Refresh all outputs before we begin\n",
    "clear_output()\n",
    "\n",
    "# Create data, logging, and model directories\n",
    "create_directory_with_gitignore(DATA_PATH)\n",
    "create_directory_with_gitignore(TOKENIZER_DIR)\n",
    "create_directory_with_gitignore(TRAINING_DIR)\n",
    "create_directory_with_gitignore(MODEL_DIR)\n",
    "create_directory_with_gitignore(LOG_DIR)\n",
    "\n",
    "# Configure tensorflow to use defined device for training\n",
    "tf.config.set_visible_devices(tf.config.list_physical_devices(DEVICE)[0], DEVICE)\n",
    "\n",
    "# Tokenize and pad the descriptions and subjects\n",
    "TOKENIZER_INPUT_PATH = os.path.join(TOKENIZER_DIR, 'tokenizer_input.pkl')\n",
    "SEQUENCES_INPUT_PATH = os.path.join(TOKENIZER_DIR, 'sequences_input.pkl')\n",
    "TOKENIZER_OUTPUT_PATH = os.path.join(TOKENIZER_DIR, 'tokenizer_output.pkl')\n",
    "SEQUENCES_OUTPUT_PATH = os.path.join(TOKENIZER_DIR, 'sequences_output.pkl')\n",
    "\n",
    "# Ask the user whether to train a new vocab or continue using existing vocab\n",
    "token_input = input(\"Enter '1' to Re-tokenize and Pad Sequences or '2' to Load Saved Files: \") # Ask the user whether to re-tokenize and pad the sequences or load the saved files\n",
    "\n",
    "# Ask the user whether to train a new model, load an existing model, or continue training from an existing model\n",
    "train_input = input_with_notification(\"Enter '1' to Train New Model, '2' to Continue Training from Existing Model, or '3' to Load Existing Model for Testing Only: \")\n",
    "\n",
    "if token_input == '1':\n",
    "    # Load and preprocess the data0\n",
    "    descriptions, subjects = load_and_preprocess_data(DATA_PATH)\n",
    "    \n",
    "    # Tokenize and pad the descriptions and subjects\n",
    "    tokenizer_input, sequences_input = tokenize_and_pad(descriptions, MAX_SEQ_LENGTH)\n",
    "    tokenizer_output, sequences_output = tokenize_and_pad(subjects, MAX_SEQ_LENGTH)\n",
    "    \n",
    "    # Dispose of the descriptions and subjects variables to free up memory load\n",
    "    del descriptions\n",
    "    del subjects\n",
    "\n",
    "    # Save tokenizers and sequences to files\n",
    "    with open(TOKENIZER_INPUT_PATH, 'wb') as f:\n",
    "        pickle.dump(tokenizer_input, f)\n",
    "    with open(SEQUENCES_INPUT_PATH, 'wb') as f:\n",
    "        pickle.dump(sequences_input, f)\n",
    "    with open(TOKENIZER_OUTPUT_PATH, 'wb') as f:\n",
    "        pickle.dump(tokenizer_output, f)\n",
    "    with open(SEQUENCES_OUTPUT_PATH, 'wb') as f:\n",
    "        pickle.dump(sequences_output, f)\n",
    "\n",
    "elif token_input == '2':\n",
    "    # Load tokenizers and sequences from files\n",
    "    with open(TOKENIZER_INPUT_PATH, 'rb') as f:\n",
    "        tokenizer_input = pickle.load(f)\n",
    "    with open(SEQUENCES_INPUT_PATH, 'rb') as f:\n",
    "        sequences_input = pickle.load(f)\n",
    "    with open(TOKENIZER_OUTPUT_PATH, 'rb') as f:\n",
    "        tokenizer_output = pickle.load(f)\n",
    "    with open(SEQUENCES_OUTPUT_PATH, 'rb') as f:\n",
    "        sequences_output = pickle.load(f)\n",
    "\n",
    "# Get the vocab sizes for the input and output\n",
    "vocab_size_input = len(tokenizer_input.word_index) + 1\n",
    "print('Input Vocab Size:', vocab_size_input)\n",
    "vocab_size_output = len(tokenizer_output.word_index) + 1\n",
    "print('Output Vocab Size:', vocab_size_output)\n",
    "\n",
    "# Create a dictionary for converting integer tokens back into words\n",
    "int_to_word_decoder = {i: word for word, i in tokenizer_output.word_index.items()}\n",
    "int_to_word_decoder[1] = '<OOV>'\n",
    "\n",
    "# Build the model\n",
    "model, encoder_input, encoder_output, state_h, state_c, decoder_input, decoder_embedding_layer, decoder_lstm, attention_layer = build_model(\n",
    "    MAX_SEQ_LENGTH, vocab_size_input, vocab_size_output, EMBEDDING_DIM)\n",
    "\n",
    "# Compile the model with a loss function, optimizer, and metrics\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Split the sequences into training and validation sets\n",
    "sequences_input_train, sequences_input_val, sequences_output_train, sequences_output_val = train_test_split(\n",
    "    sequences_input, np.expand_dims(sequences_output, -1), test_size=TEST_SPLIT_SIZE, random_state=RANDOM_STATE)\n",
    "\n",
    "# Create directories for saving model weights if they do not exist\n",
    "if not os.path.exists(os.path.dirname(ENCODER_PATH)):\n",
    "    os.makedirs(os.path.dirname(ENCODER_PATH))\n",
    "if not os.path.exists(os.path.dirname(DECODER_PATH)):\n",
    "    os.makedirs(os.path.dirname(DECODER_PATH))\n",
    "\n",
    "# Define TensorBoard and early stopping callbacks\n",
    "tensorboard_callback = TensorBoard(log_dir=LOG_DIR)\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=EARLY_STOPPING_PATIENCE)\n",
    "\n",
    "# Define the encoder and decoder models for inference\n",
    "encoder_model = Model(encoder_input, [encoder_output, state_h, state_c])  # Create an encoder model for inference, taking encoder_input as input and outputting encoder_output, state_h, and state_c\n",
    "decoder_state_input_h = Input(shape=(LSTM_UNITS*2,))  # Define the input layer for the decoder LSTM's hidden state\n",
    "decoder_state_input_c = Input(shape=(LSTM_UNITS*2,))  # Define the input layer for the decoder LSTM's cell state\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]  # Collect the decoder states inputs into a list\n",
    "decoder_hidden_state_input = Input(shape=(MAX_SEQ_LENGTH, LSTM_UNITS*2))  # Define the input layer for the decoder's hidden state\n",
    "decoder_input_inf = Input(shape=(1,))  # Define the input layer for the decoder during inference\n",
    "decoder_embedding_inf = decoder_embedding_layer(decoder_input_inf)  # Apply the decoder's embedding layer to the decoder input during inference\n",
    "context_vector, _ = attention_layer(decoder_hidden_state_input, decoder_state_input_h)  # Calculate the context vector and attention weights using the attention layer\n",
    "decoder_concat_input = Concatenate(axis=-1)([decoder_embedding_inf, tf.expand_dims(context_vector, 1)])  # Concatenate the embedding input and context vector\n",
    "decoder_outputs_inf, state_h_inf, state_c_inf = decoder_lstm(decoder_concat_input, initial_state=decoder_states_inputs)  # Pass the concatenated input and decoder states to the decoder LSTM and get the outputs and new states during inference\n",
    "decoder_states_inf = [state_h_inf, state_c_inf]  # Collect the decoder states outputs into a list\n",
    "decoder_outputs_inf = Dense(vocab_size_output, activation='softmax')(decoder_outputs_inf)  # Apply a dense layer with softmax activation to the decoder outputs during inference\n",
    "decoder_model = Model([decoder_input_inf] + decoder_states_inputs + [decoder_hidden_state_input], [decoder_outputs_inf] + decoder_states_inf)  # Create a decoder model for inference, taking decoder_input_inf, decoder_states_inputs, and decoder_hidden_state_input as inputs, and outputting decoder_outputs_inf and decoder_states_inf\n",
    "\n",
    "# Define the model checkpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(encoder_model, decoder_model)\n",
    "\n",
    "# If the user chose to train a new model, fit the model and save the best weights\n",
    "if train_input == '1':\n",
    "    # Create a BLEU score callback object with specified parameters\n",
    "    bleu_score_callback = BleuScoreCallback(tokenizer_output, sequences_input_val, sequences_output_val, BLEU_TEST_COUNT, LOG_DIR)\n",
    "    # Begin training\n",
    "    model.fit([sequences_input_train, sequences_output_train], sequences_output_train,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            epochs=NUM_EPOCHS,\n",
    "            validation_data=([sequences_input_val, sequences_output_val], sequences_output_val),\n",
    "            callbacks=[checkpoint_callback, tensorboard_callback, bleu_score_callback, early_stopping_callback])\n",
    "\n",
    "# If the user chose to continue training from an existing model, load the saved weights and continue training\n",
    "# Please be aware that this requires you to have the exact same architecture as the one that was used when the model was saved\n",
    "elif train_input == '2':\n",
    "    # Load existing weights\n",
    "    encoder_model.load_weights(ENCODER_TRAINING_PATH)\n",
    "    decoder_model.load_weights(DECODER_TRAINING_PATH)\n",
    "    # Create a BLEU score callback object with specified parameters\n",
    "    bleu_score_callback = BleuScoreCallback(tokenizer_output, sequences_input_val, sequences_output_val, BLEU_TEST_COUNT, LOG_DIR)\n",
    "    # Begin training\n",
    "    model.fit([sequences_input_train, sequences_output_train], sequences_output_train,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            epochs=NUM_EPOCHS,\n",
    "            validation_data=([sequences_input_val, sequences_output_val], sequences_output_val),\n",
    "            callbacks=[checkpoint_callback, tensorboard_callback, bleu_score_callback, early_stopping_callback])\n",
    "\n",
    "# If the user chose to load an existing model for testing, load the saved weights\n",
    "elif train_input == '3':\n",
    "    encoder_model.load_weights(ENCODER_PATH)\n",
    "    decoder_model.load_weights(DECODER_PATH)\n",
    "\n",
    "# Ask the user whether to train a new model, load an existing model, or continue training from an existing model\n",
    "test_input = input_with_notification(\"Enter '1' to Get HTML From URL or '2' to Manually Input HTML: \")\n",
    "\n",
    "if test_input == '1':\n",
    "    while True:\n",
    "        # Ask the user for a URL\n",
    "        url = input_with_notification(\"Enter a URL (type 'quit' to exit): \")\n",
    "        # Break the loop if the user types 'quit'\n",
    "        if url.lower() == 'quit':\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            # Send a GET request to the URL and retrieve the HTML content\n",
    "            response = requests.get(url)\n",
    "            html_content = response.text\n",
    "\n",
    "            # Tokenize and pad the HTML content\n",
    "            input_sequence = tokenizer_input.texts_to_sequences([html_content])\n",
    "            input_sequence = pad_sequences(input_sequence, maxlen=MAX_SEQ_LENGTH, padding='post')\n",
    "\n",
    "            # Decode the model's output sequence\n",
    "            predicted_sequence = decode_sequence(input_sequence)\n",
    "\n",
    "            # Print the predicted subject\n",
    "            print('Predicted Subject:', predicted_sequence)\n",
    "        except requests.exceptions.RequestException:\n",
    "            print('Error: Failed to retrieve HTML content from the URL.')\n",
    "\n",
    "elif test_input == '2':\n",
    "    # Loop for getting user input and predicting the output\n",
    "    while True:\n",
    "        # Ask the user for a description\n",
    "        input_description = input_with_notification(\"Enter a new description (type 'quit' to exit): \")\n",
    "        # Break the loop if the user types 'quit'\n",
    "        if input_description.lower() == 'quit': break\n",
    "        # Tokenize and pad the user's description\n",
    "        input_sequence = tokenizer_input.texts_to_sequences([input_description])\n",
    "        input_sequence = pad_sequences(input_sequence, maxlen=MAX_SEQ_LENGTH, padding='post')\n",
    "        # Decode the model's output sequence\n",
    "        predicted_sequence = decode_sequence(input_sequence)\n",
    "        # Print the predicted subject\n",
    "        print('Predicted Subject:', predicted_sequence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
