{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import os\n",
    "import IPython\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of sitemap URLs\n",
    "sitemap_urls = [\n",
    "    \"https://www.encyclopedia.com/sites/default/files/sitemaps/sitemap-articles.xml\",\n",
    "    \"https://www.encyclopedia.com/sites/default/files/sitemaps/sitemap-articles-1.xml\",\n",
    "    \"https://www.encyclopedia.com/sites/default/files/sitemaps/sitemap-articles-2.xml\",\n",
    "    \"https://www.encyclopedia.com/sites/default/files/sitemaps/sitemap-articles-3.xml\",\n",
    "    \"https://www.encyclopedia.com/sites/default/files/sitemaps/sitemap-articles-4.xml\",\n",
    "    \"https://www.encyclopedia.com/sites/default/files/sitemaps/sitemap-articles-5.xml\",\n",
    "    \"https://www.encyclopedia.com/sites/default/files/sitemaps/sitemap-articles-6.xml\",\n",
    "    \"https://www.encyclopedia.com/sites/default/files/sitemaps/sitemap-articles-7.xml\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract URLs from XML response\n",
    "def extract_urls_from_xml(xml_content):\n",
    "    urls = []\n",
    "    root = ET.fromstring(xml_content)\n",
    "    for child in root:\n",
    "        for sub_child in child:\n",
    "            if sub_child.tag.endswith(\"loc\"):\n",
    "                urls.append(sub_child.text)\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify export path\n",
    "export_path = \"C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Encyclopedia_Urls.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CSV file\n",
    "csv_header = [\"URL\"]\n",
    "with open(export_path, \"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(csv_header)\n",
    "\n",
    "    # Loop through sitemap URLs\n",
    "    for sitemap_url in sitemap_urls:\n",
    "        response = requests.get(sitemap_url)\n",
    "        if response.status_code == 200:\n",
    "            urls = extract_urls_from_xml(response.content)\n",
    "            print(\"URLs found in\", sitemap_url, \":\")\n",
    "            for url in urls:\n",
    "                print(url)\n",
    "                writer.writerow([url])\n",
    "            print()\n",
    "        else:\n",
    "            print(\"Error:\", response.status_code, \"for\", sitemap_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"URLs exported to\", export_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify export paths\n",
    "sitemap_urls_csv = export_path\n",
    "text_output_folder = \"C:/Users/jjbor/Documents/Machine Learning/Training_Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_43.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_44.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_45.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_46.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_47.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_48.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_49.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_50.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_51.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_52.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_53.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_54.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_55.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_56.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_57.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_58.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_59.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_60.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_61.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_62.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_63.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_64.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_65.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_66.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_67.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_68.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_69.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_70.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_71.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_72.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_73.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_74.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_75.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_76.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_77.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_78.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_79.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_80.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_81.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_82.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_83.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_84.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_85.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_86.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_87.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_88.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_89.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_90.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_91.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_92.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_93.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_94.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_95.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_96.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_97.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_98.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_99.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_100.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_101.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_102.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_103.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_104.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_105.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_106.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_107.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_108.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_109.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_110.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_111.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_112.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_113.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_114.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_115.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_116.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_117.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_118.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_119.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_120.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_121.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_122.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_123.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_124.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_125.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_126.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_127.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_128.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_129.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_130.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_131.txt\n",
      "Chunk exported to C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data\\376388_132.txt\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file with exported URLs\n",
    "with open(sitemap_urls_csv, \"r\") as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    # Skip header row\n",
    "    next(csv_reader)\n",
    "\n",
    "    # Initialize the increment counter\n",
    "    # Use a mutable object to hold the count variable\n",
    "    count = [0]  \n",
    "    # Create a ThreadPoolExecutor with max_workers set to 10\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "\n",
    "        def process_url(row):\n",
    "            url = row[0]\n",
    "\n",
    "            try:\n",
    "                # Send a GET request to the URL\n",
    "                response = requests.get(url)\n",
    "                # Raise an exception for failed requests\n",
    "                response.raise_for_status()  \n",
    "\n",
    "                html_content = response.content\n",
    "\n",
    "                # Extract plain text from the HTML element\n",
    "                soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "                div_element = soup.find(\"div\", class_=\"doccontentwrapper collapse show\")\n",
    "                if div_element:\n",
    "                    extracted_text = div_element.get_text()\n",
    "\n",
    "                    # Print the extracted text\n",
    "                    print(\"Extracted text from\", url, \":\")\n",
    "                    print(extracted_text)\n",
    "                    print()\n",
    "\n",
    "                    # Break up the text if longer than 1024 characters with a 96-character overlap\n",
    "                    max_length = 1024\n",
    "                    overlap = 96\n",
    "                    num_chunks = (len(extracted_text) - max_length) // (max_length - overlap) + 1\n",
    "\n",
    "                    for i in range(num_chunks):\n",
    "                        start_idx = i * (max_length - overlap)\n",
    "                        end_idx = start_idx + max_length\n",
    "\n",
    "                        chunk = extracted_text[start_idx:end_idx]\n",
    "\n",
    "                        # Export the chunk to a text file with incrementing numbering\n",
    "                        output_filename = str(count[0]) + f\"_{i}.txt\"\n",
    "                        output_path = os.path.join(text_output_folder, output_filename)\n",
    "                        with open(output_path, \"w\") as output_file:\n",
    "                            output_file.write(chunk)\n",
    "\n",
    "                        print(\"Chunk exported to\", output_path)\n",
    "\n",
    "                    # Increment the counter\n",
    "                    count[0] += 1\n",
    "                else:\n",
    "                    print(\"No matching HTML element found in\", url)\n",
    "            except Exception as e:\n",
    "                # Handle request/extraction failures silently\n",
    "                print(\"Error processing\", url + \":\", str(e))\n",
    "\n",
    "            IPython.display.clear_output(wait=True)\n",
    "\n",
    "        # Specify if URLs should be limited and by how much\n",
    "        limit_urls, max_urls = False, 10\n",
    "\n",
    "        # Submit URL processing tasks to the ThreadPoolExecutor\n",
    "        for i, row in enumerate(csv_reader):\n",
    "            if limit_urls and i >= max_urls:\n",
    "                break\n",
    "            executor.submit(process_url, row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction and export completed.\n"
     ]
    }
   ],
   "source": [
    "print(\"Extraction and export completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
