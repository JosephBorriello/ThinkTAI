{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import IPython\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the folder path containing the text files\n",
    "folder_path = \"C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Training_Data/\"\n",
    "\n",
    "# Get the paths of all .txt files in the folder\n",
    "data_files = glob.glob(os.path.join(folder_path, \"*.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the loaded examples\n",
    "examples = []\n",
    "\n",
    "# Boolean variable to determine whether to load all files\n",
    "load_all_files = False\n",
    "\n",
    "# Specify the upper limit for the number of files to load\n",
    "# Will only apply if load_all_files = False\n",
    "max_files = 5\n",
    "\n",
    "# Load examples from each file based on the specified condition\n",
    "if load_all_files:\n",
    "    for data_file in data_files:\n",
    "        with open(data_file, \"r\") as file:\n",
    "            examples.extend(file.readlines())\n",
    "else:\n",
    "    for i, data_file in enumerate(data_files):\n",
    "        if i >= max_files:\n",
    "            break\n",
    "        with open(data_file, \"r\") as file:\n",
    "            examples.extend(file.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the maximum length among all examples\n",
    "max_length = max(len(example) for example in examples)\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pad or truncate examples to the maximum length\n",
    "# for i in range(len(examples)):\n",
    "#     example = examples[i]\n",
    "#     print(len(example))\n",
    "#     if len(example) < max_length:\n",
    "#         # Pad the example with spaces at the end\n",
    "#         examples[i] = example.rstrip() + \" \" * (max_length - len(example))\n",
    "#     elif len(example) > max_length:\n",
    "#         # Truncate the example to the maximum length\n",
    "#         examples[i] = example[:max_length]\n",
    "#     print(len(examples[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the TextDataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, examples, tokenizer):\n",
    "        self.examples = examples\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples[\"input_ids\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.examples[\"input_ids\"][idx]\n",
    "        attention_mask = self.examples[\"attention_mask\"][idx]\n",
    "        return input_ids, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"C:/Users/jjbor/Documents/GitHub/ThinkTAI/ThinkTAI-GPT2/Model\"\n",
    "\n",
    "try:\n",
    "    # Try to load the saved model\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_path, padding=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"Successfully loaded the existing model.\")\n",
    "except:\n",
    "    # Fallback to the GPT2 model if loading fails\n",
    "    print(\"Failed to load the existing model. Fallback to the GPT2 model.\")\n",
    "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", padding=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad or truncate examples to the maximum length using the tokenizer's padding method\n",
    "padded_examples = tokenizer(examples, padding=True, truncation=True, max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "num_epochs = 1\n",
    "learning_rate = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TextDataset(padded_examples, tokenizer)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for input_ids, attention_mask in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        # Convert input_ids and attention_mask to tensors\n",
    "        input_ids = torch.tensor(input_ids).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "        attention_mask = torch.tensor(attention_mask).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "        labels = input_ids.clone()\n",
    "        labels[labels == tokenizer.pad_token_id] = -100\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved \n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    user_input = input(\"Enter your question (or 'q' to quit): \")\n",
    "    IPython.display.clear_output(wait=True)\n",
    "    if user_input.lower() == \"q\":\n",
    "        break\n",
    "    encoded_input = tokenizer.encode_plus(user_input, return_tensors=\"pt\")\n",
    "    input_ids = encoded_input.input_ids.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    attention_mask = encoded_input.attention_mask.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    output = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=100, num_return_sequences=1)\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(\"Response:\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
